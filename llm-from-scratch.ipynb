{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12913961,"sourceType":"datasetVersion","datasetId":8171298}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport tiktoken\nfrom glob import glob\nimport torch.nn as nn\nfrom torch.nn import functional as F, Embedding\nfrom torch.utils.data import Dataset, DataLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:46.317694Z","iopub.execute_input":"2025-09-04T18:02:46.317927Z","iopub.status.idle":"2025-09-04T18:02:50.473890Z","shell.execute_reply.started":"2025-09-04T18:02:46.317909Z","shell.execute_reply":"2025-09-04T18:02:50.473298Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"class Config:\n    device = \"cuda:0\"\n    n_vocab = 50257\n    d_model = 768\n    max_length = 256\n    stride = 256\n    qkv_bias = False\n    embedding_dropout = 0\n    num_head = 16\n    multihead_attention_dropout = 0\n    num_of_stacked_transformer_blocks = 24","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:50.475041Z","iopub.execute_input":"2025-09-04T18:02:50.475362Z","iopub.status.idle":"2025-09-04T18:02:50.479417Z","shell.execute_reply.started":"2025-09-04T18:02:50.475343Z","shell.execute_reply":"2025-09-04T18:02:50.478751Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"text_path = glob(\"/kaggle/input/text-data\"+\"/**.txt\")[0]\n\nwith open(text_path, \"r\") as text_file:\n    text_file_read = text_file.read()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:50.479999Z","iopub.execute_input":"2025-09-04T18:02:50.480231Z","iopub.status.idle":"2025-09-04T18:02:50.505198Z","shell.execute_reply.started":"2025-09-04T18:02:50.480211Z","shell.execute_reply":"2025-09-04T18:02:50.504523Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"tokenizer = tiktoken.get_encoding(encoding_name=\"gpt2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:50.506693Z","iopub.execute_input":"2025-09-04T18:02:50.506887Z","iopub.status.idle":"2025-09-04T18:02:54.569312Z","shell.execute_reply.started":"2025-09-04T18:02:50.506871Z","shell.execute_reply":"2025-09-04T18:02:54.568741Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class GPTDataset(Dataset):\n    def __init__(self, text, tokenizer, max_length, stride):\n        self.X_out, self.y_out = [], []\n        self.token_ids = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n        assert len(self.token_ids)>max_length, f\"Length of Token IDs {len(self.token_ids)} not greater than Max length {max_length}\"\n\n        for idx in range(0, len(self.token_ids)-max_length, stride):\n            X, y = self.token_ids[idx:idx+max_length], self.token_ids[idx+1:idx+max_length+1]\n            self.X_out.append(X)\n            self.y_out.append(y)\n        \n    def __len__(self):\n        return len(self.X_out)\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.X_out[idx]), torch.tensor(self.y_out[idx])\n\ndef CreateDataLoader(data, tokenizer, cfg, batch_size, shuffle, drop_last, num_workers):\n    dataset = GPTDataset(text=data, tokenizer=tokenizer, max_length=cfg.max_length, stride=cfg.stride)\n    data_loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n    return data_loader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:54.570438Z","iopub.execute_input":"2025-09-04T18:02:54.570707Z","iopub.status.idle":"2025-09-04T18:02:54.576902Z","shell.execute_reply.started":"2025-09-04T18:02:54.570677Z","shell.execute_reply":"2025-09-04T18:02:54.576122Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"train_size = .9\ntotal_tokens = len(text_file_read)\ntrain_ratio = int(train_size * total_tokens)\ntrain_text = text_file_read[:train_ratio]\nval_text = text_file_read[train_ratio:]\n\nprint(\"Train Length:\", len(train_text), \", Val Length:\", len(val_text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:54.577572Z","iopub.execute_input":"2025-09-04T18:02:54.577740Z","iopub.status.idle":"2025-09-04T18:02:54.592728Z","shell.execute_reply.started":"2025-09-04T18:02:54.577726Z","shell.execute_reply":"2025-09-04T18:02:54.592141Z"}},"outputs":[{"name":"stdout","text":"Train Length: 18431 , Val Length: 2048\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"train_dl = CreateDataLoader(train_text, tokenizer, Config, 2, True, True, os.cpu_count())\nval_dl = CreateDataLoader(val_text, tokenizer, Config, 2, False, False, os.cpu_count())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:54.593342Z","iopub.execute_input":"2025-09-04T18:02:54.593594Z","iopub.status.idle":"2025-09-04T18:02:54.609464Z","shell.execute_reply.started":"2025-09-04T18:02:54.593568Z","shell.execute_reply":"2025-09-04T18:02:54.608841Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, cfg):\n        super(MultiHeadAttention, self).__init__()\n        self.cfg = cfg\n        assert cfg.d_model % cfg.num_head == 0, \"cfg.d_model % cfg.num_head != 0\"\n        self.d_k = cfg.d_model // cfg.num_head\n        # Attentions Projections\n        self.W_q = nn.Linear(cfg.d_model, cfg.d_model, bias=cfg.qkv_bias)\n        self.W_k = nn.Linear(cfg.d_model, cfg.d_model, bias=cfg.qkv_bias)\n        self.W_v = nn.Linear(cfg.d_model, cfg.d_model, bias=cfg.qkv_bias)\n        self.W_o = nn.Linear(cfg.d_model, cfg.d_model, bias=cfg.qkv_bias)\n        # Dropout\n        self.dropout = nn.Dropout(p=cfg.multihead_attention_dropout)\n\n    def forward(self, x):\n        batch_size, seq, d_model = x.size()\n        q = self.W_q(x).view(batch_size, seq, self.cfg.num_head, self.d_k).transpose(1, 2)\n        k = self.W_k(x).view(batch_size, seq, self.cfg.num_head, self.d_k).transpose(1, 2)\n        v = self.W_v(x).view(batch_size, seq, self.cfg.num_head, self.d_k).transpose(1, 2)\n\n        mask = torch.tril(torch.ones(seq, seq, device=x.device)).unsqueeze(0).unsqueeze(0)\n        attention_score = q @ k.transpose(-2, -1) / torch.sqrt(torch.tensor(self.d_k))\n        attention = self.dropout(F.softmax(attention_score.masked_fill(mask==0, -torch.inf), dim=-1)) @ v # dropout(softmax(attention score  + mask)) @ v, dim=(batch_size, num_head, seq_len, d_k)\n        output = attention.transpose(1, 2).contiguous().view(batch_size, seq, d_model) # (batch_size, seq, d_model)\n        return self.W_o(output) # (batch_size, seq, d_model)\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n            (x + 0.044715 * (x**3)))\n        )\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super(FeedForward, self).__init__()\n        self.cfg = cfg\n        self.ff = nn.Sequential(\n            nn.Linear(self.cfg.d_model, 4*self.cfg.d_model),\n            GELU(),\n            nn.Linear(4*self.cfg.d_model, self.cfg.d_model)\n        )\n\n    def forward(self, x):\n        return self.ff(x)\n\nclass LayerNormalization(nn.Module):\n    def __init__(self, cfg):\n        super(LayerNormalization, self).__init__()\n        self.eps = 1e-5\n        self.Weight = nn.Parameter(torch.ones(cfg.d_model))\n        self.bias = nn.Parameter(torch.zeros(cfg.d_model))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        variance = x.var(dim=-1, keepdim=True, unbiased=False)\n        return self.Weight * (x - mean / torch.sqrt(variance+self.eps)) + self.bias\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super(TransformerBlock, self).__init__()\n        self.multihead_attention = MultiHeadAttention(cfg)\n        self.layer_norm_1 = LayerNormalization(cfg)\n        self.layer_norm_2 = LayerNormalization(cfg)\n        self.dropout = nn.Dropout(p=cfg.multihead_attention_dropout)\n        self.feedforward = FeedForward(cfg)\n\n    def forward(self, x):\n        shortcut = x\n        x = self.layer_norm_1(x)\n        x = self.multihead_attention(x)\n        x = self.dropout(x)\n        x = x + shortcut\n\n        shortcut = x\n        x = self.layer_norm_2(x)\n        x = self.feedforward(x)\n        x = self.dropout(x)\n        x = x + shortcut\n        return x # (batch_size, seq, d_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:54.610114Z","iopub.execute_input":"2025-09-04T18:02:54.610369Z","iopub.status.idle":"2025-09-04T18:02:54.623804Z","shell.execute_reply.started":"2025-09-04T18:02:54.610341Z","shell.execute_reply":"2025-09-04T18:02:54.623155Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class GptModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.cfg = cfg\n        \n        # Embeddings\n        self.wrd_embeddings = nn.Embedding(cfg.n_vocab, cfg.d_model)\n        self.pos_embeddings = nn.Embedding(cfg.max_length, cfg.d_model)\n        self.embedding_dropout = nn.Dropout(p=cfg.embedding_dropout)\n\n        # Stacked Multihead Attention\n        # nn.Sequential([Block1, Block2, Block3])  # ‚ùå wrong thats why the * so that it will unpack the list into nn.Sequential(Block1, Block2, Block3)\n        self.stack_mha = nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg.num_of_stacked_transformer_blocks)])\n\n        # Final layer norm\n        self.final_layer_norm = LayerNormalization(cfg)\n\n        # Output layer\n        self.output_layer = nn.Linear(cfg.d_model, cfg.n_vocab)\n\n    def forward(self, x):\n        batch, seq = x.shape\n        wrd_embed = self.wrd_embeddings(x)\n        positions = torch.arange(seq, device=x.device).unsqueeze(0).expand(batch, -1) # (seq,) -> (1, seq) -> (batch, seq)\n        pos_embed = self.pos_embeddings(positions)\n        xout = self.embedding_dropout(wrd_embed + pos_embed)\n        xout = self.stack_mha(xout) # (batch_size, seq, d_model)\n        xout = self.final_layer_norm(xout) # (batch_size, seq, d_model)\n        return self.output_layer(xout) # (batch_size, seq, n_vocabs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:54.624411Z","iopub.execute_input":"2025-09-04T18:02:54.624637Z","iopub.status.idle":"2025-09-04T18:02:54.636498Z","shell.execute_reply.started":"2025-09-04T18:02:54.624614Z","shell.execute_reply":"2025-09-04T18:02:54.635735Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"gpt_model = GptModel(cfg=Config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:54.638583Z","iopub.execute_input":"2025-09-04T18:02:54.638756Z","iopub.status.idle":"2025-09-04T18:02:56.770413Z","shell.execute_reply.started":"2025-09-04T18:02:54.638735Z","shell.execute_reply":"2025-09-04T18:02:56.769791Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def GenerateText(model, token_ids, max_new_tokens, context_size, tokenizer):\n    for _ in range(max_new_tokens):\n        tkn_ids = token_ids[:, -context_size:]\n        with torch.no_grad():\n            logits = model(tkn_ids) # (batch_size, seq, n_vocabs)\n        last_token_pred = F.softmax(logits[:, -1, :], dim=-1) # (batch_size, seq, n_vocabs)\n        max_prob = torch.argmax(last_token_pred, dim=-1, keepdim=True) \n        token_ids = torch.cat((token_ids, max_prob), dim=1)\n    return tokenizer.decode(token_ids.tolist()[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:56.771033Z","iopub.execute_input":"2025-09-04T18:02:56.771246Z","iopub.status.idle":"2025-09-04T18:02:56.776120Z","shell.execute_reply.started":"2025-09-04T18:02:56.771230Z","shell.execute_reply":"2025-09-04T18:02:56.775363Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"input_txt = \"Hello my name is Eddy\"\ntkn_id = torch.tensor(tokenizer.encode(input_txt)).unsqueeze(0).to(\"cuda:0\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:56.776713Z","iopub.execute_input":"2025-09-04T18:02:56.776893Z","iopub.status.idle":"2025-09-04T18:02:57.019902Z","shell.execute_reply.started":"2025-09-04T18:02:56.776878Z","shell.execute_reply":"2025-09-04T18:02:57.019143Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"model_output = GenerateText(model=gpt_model.to(\"cuda:0\"), token_ids=tkn_id, max_new_tokens=50, context_size=50, tokenizer=tokenizer)\n\nmodel_output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:57.020929Z","iopub.execute_input":"2025-09-04T18:02:57.021185Z","iopub.status.idle":"2025-09-04T18:02:59.196497Z","shell.execute_reply.started":"2025-09-04T18:02:57.021167Z","shell.execute_reply":"2025-09-04T18:02:59.195930Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'Hello my name is Eddy $_ $_ Purpose Human NFCukedOrg professionalism $_ cognitionWINDOWS Zhu July chilly unions thermal youngrank frontman $_uits Purposeamia professionalism Fischer Fischer Riverside fins frontman UNDER Fischeruncle chilly AnswersWINDOWS Alph mailbox Purpose brazenrank professionalismOrg orbital Purpose Fischer thermal Honest NarendraemnESA'"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"class Trainer:\n    def __init__(self, model, train_dl, val_dl, optimizer, device):\n        self.model = model.to(device)\n        self.optimizer = optimizer\n        self.device = device\n        self.train_dl = train_dl\n        self.val_dl = val_dl\n\n    def train(self, train_dl):\n        total_loss = 0\n        self.model.train()\n        for x, y in train_dl:\n            self.optimizer.zero_grad()\n            x, y = x.to(self.device), y.to(self.device)\n            logits = self.model(x) # (batch_size, seq_len, n_vocabs)\n            loss = F.cross_entropy(logits.flatten(0, 1), y.flatten()) #logits.flatten(0, 1) -> (batch_size*seq_len, n_vocabs), y.flatten() -> (batch_size*seq_len,)\n            loss.backward()\n            self.optimizer.step()\n            total_loss+=loss.item()\n        return total_loss\n\n    def eval(self, val_dl):\n        total_loss = 0\n        self.model.eval()\n        for x, y in val_dl:\n            with torch.no_grad():\n                x, y = x.to(self.device), y.to(self.device)\n                logits = self.model(x)\n                loss = F.cross_entropy(logits.flatten(0, 1), y.flatten())\n                total_loss+=loss.item()\n        return total_loss\n\n    def fit(self, n_epochs):\n        display_models_output = 5\n        for epoch in range(n_epochs):\n            train_loss = self.train(self.train_dl)\n            eval_loss = self.eval(self.val_dl)\n    \n            print(f\"Epoch {epoch+1}/{n_epochs} -- Train loss: {train_loss:.4f} -- Val loss: {eval_loss:.4f}\")\n    \n            if (epoch + 1) % display_models_output == 0:\n                input_txt = \"I HAD always thought Jack Gisburn rather\"\n                tkn_id = torch.tensor(tokenizer.encode(input_txt)).unsqueeze(0).to(self.device)\n                output = GenerateText(\n                    model=self.model,\n                    token_ids=tkn_id,\n                    max_new_tokens=30,\n                    context_size=30,\n                    tokenizer=tokenizer\n                )\n                print(output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:59.197119Z","iopub.execute_input":"2025-09-04T18:02:59.197311Z","iopub.status.idle":"2025-09-04T18:02:59.205875Z","shell.execute_reply.started":"2025-09-04T18:02:59.197292Z","shell.execute_reply":"2025-09-04T18:02:59.205347Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(params=gpt_model.parameters(), lr=4e-5, weight_decay=.1)\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:02:59.206628Z","iopub.execute_input":"2025-09-04T18:02:59.206877Z","iopub.status.idle":"2025-09-04T18:03:01.683323Z","shell.execute_reply.started":"2025-09-04T18:02:59.206855Z","shell.execute_reply":"2025-09-04T18:03:01.682747Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"trainer = Trainer(model=gpt_model, train_dl=train_dl, val_dl=val_dl, optimizer=optimizer, device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:03:01.684041Z","iopub.execute_input":"2025-09-04T18:03:01.684352Z","iopub.status.idle":"2025-09-04T18:03:01.691475Z","shell.execute_reply.started":"2025-09-04T18:03:01.684334Z","shell.execute_reply":"2025-09-04T18:03:01.690778Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"trainer.fit(n_epochs=100)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T18:03:01.692182Z","iopub.execute_input":"2025-09-04T18:03:01.692374Z","iopub.status.idle":"2025-09-04T18:06:33.054108Z","shell.execute_reply.started":"2025-09-04T18:03:01.692358Z","shell.execute_reply":"2025-09-04T18:06:33.053150Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100 -- Train loss: 114.9098 -- Val loss: 11.1948\nEpoch 2/100 -- Train loss: 94.0959 -- Val loss: 10.6217\nEpoch 3/100 -- Train loss: 78.1829 -- Val loss: 9.8181\nEpoch 4/100 -- Train loss: 64.6679 -- Val loss: 8.9218\nEpoch 5/100 -- Train loss: 51.7605 -- Val loss: 8.3731\nI HAD always thought Jack Gisburn rather think so Mrs- his pictures isI- not- inrah- inrah- in his pictures it and my that, and I asI I\nEpoch 6/100 -- Train loss: 41.1645 -- Val loss: 8.1469\nEpoch 7/100 -- Train loss: 30.3654 -- Val loss: 7.9971\nEpoch 8/100 -- Train loss: 21.6356 -- Val loss: 8.0106\nEpoch 9/100 -- Train loss: 14.7735 -- Val loss: 7.7961\nEpoch 10/100 -- Train loss: 9.5650 -- Val loss: 7.9010\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that he was to put it to those such donkey\nEpoch 11/100 -- Train loss: 4.8632 -- Val loss: 7.9052\nEpoch 12/100 -- Train loss: 2.1169 -- Val loss: 7.9797\nEpoch 13/100 -- Train loss: 0.9599 -- Val loss: 7.9936\nEpoch 14/100 -- Train loss: 0.4065 -- Val loss: 7.9727\nEpoch 15/100 -- Train loss: 0.2018 -- Val loss: 8.0147\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and down all it--as, and it\nEpoch 16/100 -- Train loss: 0.1098 -- Val loss: 7.9687\nEpoch 17/100 -- Train loss: 0.0661 -- Val loss: 7.9794\nEpoch 18/100 -- Train loss: 0.0508 -- Val loss: 8.0031\nEpoch 19/100 -- Train loss: 0.0392 -- Val loss: 8.0066\nEpoch 20/100 -- Train loss: 0.0331 -- Val loss: 8.0079\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he her. place\nEpoch 21/100 -- Train loss: 0.0292 -- Val loss: 8.0105\nEpoch 22/100 -- Train loss: 0.0263 -- Val loss: 8.0159\nEpoch 23/100 -- Train loss: 0.0242 -- Val loss: 8.0220\nEpoch 24/100 -- Train loss: 0.0224 -- Val loss: 8.0273\nEpoch 25/100 -- Train loss: 0.0210 -- Val loss: 8.0330\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it to me.\n\n\nEpoch 26/100 -- Train loss: 0.0197 -- Val loss: 8.0380\nEpoch 27/100 -- Train loss: 0.0186 -- Val loss: 8.0413\nEpoch 28/100 -- Train loss: 0.0176 -- Val loss: 8.0449\nEpoch 29/100 -- Train loss: 0.0167 -- Val loss: 8.0488\nEpoch 30/100 -- Train loss: 0.0159 -- Val loss: 8.0520\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he her. place\nEpoch 31/100 -- Train loss: 0.0152 -- Val loss: 8.0551\nEpoch 32/100 -- Train loss: 0.0145 -- Val loss: 8.0581\nEpoch 33/100 -- Train loss: 0.0139 -- Val loss: 8.0615\nEpoch 34/100 -- Train loss: 0.0133 -- Val loss: 8.0638\nEpoch 35/100 -- Train loss: 0.0128 -- Val loss: 8.0664\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he her. place\nEpoch 36/100 -- Train loss: 0.0123 -- Val loss: 8.0689\nEpoch 37/100 -- Train loss: 0.0118 -- Val loss: 8.0713\nEpoch 38/100 -- Train loss: 0.0114 -- Val loss: 8.0740\nEpoch 39/100 -- Train loss: 0.0110 -- Val loss: 8.0761\nEpoch 40/100 -- Train loss: 0.0106 -- Val loss: 8.0785\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he her. place\nEpoch 41/100 -- Train loss: 0.0103 -- Val loss: 8.0809\nEpoch 42/100 -- Train loss: 0.0100 -- Val loss: 8.0831\nEpoch 43/100 -- Train loss: 0.0097 -- Val loss: 8.0853\nEpoch 44/100 -- Train loss: 0.0094 -- Val loss: 8.0874\nEpoch 45/100 -- Train loss: 0.0091 -- Val loss: 8.0891\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he her. place\nEpoch 46/100 -- Train loss: 0.0088 -- Val loss: 8.0905\nEpoch 47/100 -- Train loss: 0.0086 -- Val loss: 8.0926\nEpoch 48/100 -- Train loss: 0.0083 -- Val loss: 8.0942\nEpoch 49/100 -- Train loss: 0.0081 -- Val loss: 8.0961\nEpoch 50/100 -- Train loss: 0.0079 -- Val loss: 8.0976\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 51/100 -- Train loss: 0.0077 -- Val loss: 8.0992\nEpoch 52/100 -- Train loss: 0.0075 -- Val loss: 8.1004\nEpoch 53/100 -- Train loss: 0.0073 -- Val loss: 8.1021\nEpoch 54/100 -- Train loss: 0.0071 -- Val loss: 8.1035\nEpoch 55/100 -- Train loss: 0.0070 -- Val loss: 8.1050\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 56/100 -- Train loss: 0.0068 -- Val loss: 8.1065\nEpoch 57/100 -- Train loss: 0.0067 -- Val loss: 8.1075\nEpoch 58/100 -- Train loss: 0.0065 -- Val loss: 8.1085\nEpoch 59/100 -- Train loss: 0.0064 -- Val loss: 8.1095\nEpoch 60/100 -- Train loss: 0.0062 -- Val loss: 8.1111\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 61/100 -- Train loss: 0.0061 -- Val loss: 8.1120\nEpoch 62/100 -- Train loss: 0.0060 -- Val loss: 8.1133\nEpoch 63/100 -- Train loss: 0.0058 -- Val loss: 8.1147\nEpoch 64/100 -- Train loss: 0.0057 -- Val loss: 8.1157\nEpoch 65/100 -- Train loss: 0.0056 -- Val loss: 8.1166\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 66/100 -- Train loss: 0.0055 -- Val loss: 8.1173\nEpoch 67/100 -- Train loss: 0.0054 -- Val loss: 8.1185\nEpoch 68/100 -- Train loss: 0.0053 -- Val loss: 8.1195\nEpoch 69/100 -- Train loss: 0.0052 -- Val loss: 8.1203\nEpoch 70/100 -- Train loss: 0.0051 -- Val loss: 8.1213\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 71/100 -- Train loss: 0.0050 -- Val loss: 8.1222\nEpoch 72/100 -- Train loss: 0.0049 -- Val loss: 8.1228\nEpoch 73/100 -- Train loss: 0.0048 -- Val loss: 8.1236\nEpoch 74/100 -- Train loss: 0.0047 -- Val loss: 8.1245\nEpoch 75/100 -- Train loss: 0.0047 -- Val loss: 8.1252\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 76/100 -- Train loss: 0.0046 -- Val loss: 8.1261\nEpoch 77/100 -- Train loss: 0.0045 -- Val loss: 8.1267\nEpoch 78/100 -- Train loss: 0.0044 -- Val loss: 8.1275\nEpoch 79/100 -- Train loss: 0.0044 -- Val loss: 8.1283\nEpoch 80/100 -- Train loss: 0.0043 -- Val loss: 8.1290\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 81/100 -- Train loss: 0.0042 -- Val loss: 8.1297\nEpoch 82/100 -- Train loss: 0.0041 -- Val loss: 8.1303\nEpoch 83/100 -- Train loss: 0.0041 -- Val loss: 8.1307\nEpoch 84/100 -- Train loss: 0.0040 -- Val loss: 8.1316\nEpoch 85/100 -- Train loss: 0.0040 -- Val loss: 8.1320\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and he had been to\nEpoch 86/100 -- Train loss: 0.0039 -- Val loss: 8.1329\nEpoch 87/100 -- Train loss: 0.0038 -- Val loss: 8.1333\nEpoch 88/100 -- Train loss: 0.0038 -- Val loss: 8.1339\nEpoch 89/100 -- Train loss: 0.0037 -- Val loss: 8.1343\nEpoch 90/100 -- Train loss: 0.0037 -- Val loss: 8.1346\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and to my. I\nEpoch 91/100 -- Train loss: 0.0036 -- Val loss: 8.1349\nEpoch 92/100 -- Train loss: 0.0036 -- Val loss: 8.1356\nEpoch 93/100 -- Train loss: 0.0035 -- Val loss: 8.1361\nEpoch 94/100 -- Train loss: 0.0035 -- Val loss: 8.1365\nEpoch 95/100 -- Train loss: 0.0034 -- Val loss: 8.1373\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and to my. I\nEpoch 96/100 -- Train loss: 0.0034 -- Val loss: 8.1378\nEpoch 97/100 -- Train loss: 0.0033 -- Val loss: 8.1381\nEpoch 98/100 -- Train loss: 0.0033 -- Val loss: 8.1384\nEpoch 99/100 -- Train loss: 0.0033 -- Val loss: 8.1386\nEpoch 100/100 -- Train loss: 0.0032 -- Val loss: 8.1389\nI HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that and followed- it and to my. I\n","output_type":"stream"}],"execution_count":17}]}