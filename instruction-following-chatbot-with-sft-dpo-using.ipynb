{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n\n!pip install unsloth trl vllm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:26.844101Z","iopub.execute_input":"2025-08-08T11:10:26.844628Z","iopub.status.idle":"2025-08-08T11:10:31.658169Z","shell.execute_reply.started":"2025-08-08T11:10:26.844602Z","shell.execute_reply":"2025-08-08T11:10:31.657138Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class Config:\n    model_name=\"Qwen/Qwen3-0.6B-Base\"\n    instruction_dataset_name=\"OpenAssistant/oasst1\"\n    dpo_dataset_name=\"HuggingFaceH4/ultrafeedback_binarized\"\n    random_state=2025","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:31.659661Z","iopub.execute_input":"2025-08-08T11:10:31.659898Z","iopub.status.idle":"2025-08-08T11:10:31.664089Z","shell.execute_reply.started":"2025-08-08T11:10:31.659875Z","shell.execute_reply":"2025-08-08T11:10:31.663420Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from warnings import filterwarnings\nfilterwarnings(action=\"ignore\")\n\nimport tqdm\nimport torch\nimport pandas as pd\nfrom peft import PeftModel\nfrom unsloth import PatchDPOTrainer\nfrom transformers import TextStreamer\nfrom datasets import load_dataset, Dataset\n## Note: for some reason, importing trl before unsloth leads to unessary errors so i imported unsloth before trl to avoid it.\n## Reference: https://stackoverflow.com/questions/79663362/sfttrainer-the-specified-eos-token-eos-token-is-not-found-in-the-vocabu\nfrom unsloth import FastLanguageModel, is_bfloat16_supported\nfrom unsloth.chat_templates import CHAT_TEMPLATES, get_chat_template\nfrom trl import SFTConfig, SFTTrainer, DPOTrainer, DPOConfig","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:31.664924Z","iopub.execute_input":"2025-08-08T11:10:31.665212Z","iopub.status.idle":"2025-08-08T11:10:31.684542Z","shell.execute_reply.started":"2025-08-08T11:10:31.665188Z","shell.execute_reply":"2025-08-08T11:10:31.683899Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"instruction_dataset = load_dataset(Config.instruction_dataset_name)\n\ninstruction_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:31.685877Z","iopub.execute_input":"2025-08-08T11:10:31.686522Z","iopub.status.idle":"2025-08-08T11:10:32.516842Z","shell.execute_reply.started":"2025-08-08T11:10:31.686503Z","shell.execute_reply":"2025-08-08T11:10:32.516114Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n        num_rows: 84437\n    })\n    validation: Dataset({\n        features: ['message_id', 'parent_id', 'user_id', 'created_date', 'text', 'role', 'lang', 'review_count', 'review_result', 'deleted', 'rank', 'synthetic', 'model_name', 'detoxify', 'message_tree_id', 'tree_state', 'emojis', 'labels'],\n        num_rows: 4401\n    })\n})"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"instruction_dataset[\"train\"][0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:32.517494Z","iopub.execute_input":"2025-08-08T11:10:32.517682Z","iopub.status.idle":"2025-08-08T11:10:32.525170Z","shell.execute_reply.started":"2025-08-08T11:10:32.517666Z","shell.execute_reply":"2025-08-08T11:10:32.524612Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'message_id': '6ab24d72-0181-4594-a9cd-deaf170242fb',\n 'parent_id': None,\n 'user_id': 'c3fe8c76-fc30-4fa7-b7f8-c492f5967d18',\n 'created_date': '2023-02-05T14:23:50.983374+00:00',\n 'text': 'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.',\n 'role': 'prompter',\n 'lang': 'en',\n 'review_count': 3,\n 'review_result': True,\n 'deleted': False,\n 'rank': None,\n 'synthetic': False,\n 'model_name': None,\n 'detoxify': {'toxicity': 0.00044308538781479,\n  'severe_toxicity': 3.252684837207198e-05,\n  'obscene': 0.00023475120542570949,\n  'identity_attack': 0.0001416115992469713,\n  'insult': 0.00039489680784754455,\n  'threat': 4.075629112776369e-05,\n  'sexual_explicit': 2.712695459194947e-05},\n 'message_tree_id': '6ab24d72-0181-4594-a9cd-deaf170242fb',\n 'tree_state': 'ready_for_export',\n 'emojis': {'name': ['+1', '_skip_reply', '_skip_ranking'],\n  'count': [10, 1, 4]},\n 'labels': {'name': ['spam',\n   'lang_mismatch',\n   'pii',\n   'not_appropriate',\n   'hate_speech',\n   'sexual_content',\n   'quality',\n   'toxicity',\n   'humor',\n   'creativity',\n   'violence'],\n  'value': [0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.0,\n   0.9166666666666666,\n   0.16666666666666666,\n   0.3333333333333333,\n   0.6666666666666666,\n   0.0],\n  'count': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}}"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"instruction_dataset_train = instruction_dataset[\"train\"].to_pandas()\ninstruction_dataset_valid = instruction_dataset[\"validation\"].to_pandas()\n\ncombined_instruction_data = pd.concat([instruction_dataset_train, instruction_dataset_valid], axis=0)\n\nprint(combined_instruction_data.shape)\n\ncombined_instruction_data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:32.525821Z","iopub.execute_input":"2025-08-08T11:10:32.526068Z","iopub.status.idle":"2025-08-08T11:10:33.246370Z","shell.execute_reply.started":"2025-08-08T11:10:32.526051Z","shell.execute_reply":"2025-08-08T11:10:33.245596Z"}},"outputs":[{"name":"stdout","text":"(88838, 18)\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"                             message_id                             parent_id  \\\n0  6ab24d72-0181-4594-a9cd-deaf170242fb                                  None   \n1  c8e83833-ecbc-44fe-b6db-735228c25a1c  6ab24d72-0181-4594-a9cd-deaf170242fb   \n2  6708c47f-05c9-4346-b3d2-40b2bd24fde4  c8e83833-ecbc-44fe-b6db-735228c25a1c   \n3  343ee2d4-87ae-41fd-a768-bdd65959dc4a  6ab24d72-0181-4594-a9cd-deaf170242fb   \n4  18145bf4-37fd-4ac0-80f5-6108b5f2b365  343ee2d4-87ae-41fd-a768-bdd65959dc4a   \n\n                                user_id                      created_date  \\\n0  c3fe8c76-fc30-4fa7-b7f8-c492f5967d18  2023-02-05T14:23:50.983374+00:00   \n1  2c96e467-66f0-4be7-9693-bda51356a424  2023-02-06T13:50:44.657083+00:00   \n2  2c96e467-66f0-4be7-9693-bda51356a424  2023-02-06T18:48:49.391686+00:00   \n3  49ddcb0d-6588-43bd-858d-19c577f12e7b  2023-02-06T13:37:56.044680+00:00   \n4  e10e99a0-38ac-4b07-bf5d-4427696e4e0d  2023-02-06T18:52:51.428543+00:00   \n\n                                                text       role lang  \\\n0  Can you write a short introduction about the r...   prompter   en   \n1  \"Monopsony\" refers to a market structure where...  assistant   en   \n2                            Now explain it to a dog   prompter   en   \n3  Monopsony is a market structure in which there...  assistant   en   \n4  How can one fight back when a monospony had be...   prompter   en   \n\n   review_count review_result  deleted  rank  synthetic model_name  \\\n0             3          True    False   NaN      False       None   \n1             3          True    False   0.0      False       None   \n2             3          True    False   NaN      False       None   \n3             3          True    False   1.0      False       None   \n4             3          True    False   NaN      False       None   \n\n                                            detoxify  \\\n0  {'toxicity': 0.00044308538781479, 'severe_toxi...   \n1  {'toxicity': 0.00026396565954200923, 'severe_t...   \n2  {'toxicity': 0.03648477792739868, 'severe_toxi...   \n3  {'toxicity': 0.0008866374846547842, 'severe_to...   \n4  {'toxicity': 0.0009362137061543763, 'severe_to...   \n\n                        message_tree_id        tree_state  \\\n0  6ab24d72-0181-4594-a9cd-deaf170242fb  ready_for_export   \n1  6ab24d72-0181-4594-a9cd-deaf170242fb  ready_for_export   \n2  6ab24d72-0181-4594-a9cd-deaf170242fb  ready_for_export   \n3  6ab24d72-0181-4594-a9cd-deaf170242fb  ready_for_export   \n4  6ab24d72-0181-4594-a9cd-deaf170242fb  ready_for_export   \n\n                                              emojis  \\\n0  {'name': ['+1', '_skip_reply', '_skip_ranking'...   \n1  {'name': ['+1', '_skip_labeling'], 'count': [3...   \n2                                               None   \n3  {'name': ['+1', '_skip_reply', '_skip_labeling...   \n4                     {'name': ['+1'], 'count': [1]}   \n\n                                              labels  \n0  {'name': ['spam', 'lang_mismatch', 'pii', 'not...  \n1  {'name': ['spam', 'fails_task', 'lang_mismatch...  \n2  {'name': ['spam', 'lang_mismatch', 'pii', 'not...  \n3  {'name': ['spam', 'fails_task', 'lang_mismatch...  \n4  {'name': ['spam', 'lang_mismatch', 'pii', 'not...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>message_id</th>\n      <th>parent_id</th>\n      <th>user_id</th>\n      <th>created_date</th>\n      <th>text</th>\n      <th>role</th>\n      <th>lang</th>\n      <th>review_count</th>\n      <th>review_result</th>\n      <th>deleted</th>\n      <th>rank</th>\n      <th>synthetic</th>\n      <th>model_name</th>\n      <th>detoxify</th>\n      <th>message_tree_id</th>\n      <th>tree_state</th>\n      <th>emojis</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>None</td>\n      <td>c3fe8c76-fc30-4fa7-b7f8-c492f5967d18</td>\n      <td>2023-02-05T14:23:50.983374+00:00</td>\n      <td>Can you write a short introduction about the r...</td>\n      <td>prompter</td>\n      <td>en</td>\n      <td>3</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>None</td>\n      <td>{'toxicity': 0.00044308538781479, 'severe_toxi...</td>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>ready_for_export</td>\n      <td>{'name': ['+1', '_skip_reply', '_skip_ranking'...</td>\n      <td>{'name': ['spam', 'lang_mismatch', 'pii', 'not...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c8e83833-ecbc-44fe-b6db-735228c25a1c</td>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>2c96e467-66f0-4be7-9693-bda51356a424</td>\n      <td>2023-02-06T13:50:44.657083+00:00</td>\n      <td>\"Monopsony\" refers to a market structure where...</td>\n      <td>assistant</td>\n      <td>en</td>\n      <td>3</td>\n      <td>True</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>False</td>\n      <td>None</td>\n      <td>{'toxicity': 0.00026396565954200923, 'severe_t...</td>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>ready_for_export</td>\n      <td>{'name': ['+1', '_skip_labeling'], 'count': [3...</td>\n      <td>{'name': ['spam', 'fails_task', 'lang_mismatch...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>6708c47f-05c9-4346-b3d2-40b2bd24fde4</td>\n      <td>c8e83833-ecbc-44fe-b6db-735228c25a1c</td>\n      <td>2c96e467-66f0-4be7-9693-bda51356a424</td>\n      <td>2023-02-06T18:48:49.391686+00:00</td>\n      <td>Now explain it to a dog</td>\n      <td>prompter</td>\n      <td>en</td>\n      <td>3</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>None</td>\n      <td>{'toxicity': 0.03648477792739868, 'severe_toxi...</td>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>ready_for_export</td>\n      <td>None</td>\n      <td>{'name': ['spam', 'lang_mismatch', 'pii', 'not...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>343ee2d4-87ae-41fd-a768-bdd65959dc4a</td>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>49ddcb0d-6588-43bd-858d-19c577f12e7b</td>\n      <td>2023-02-06T13:37:56.044680+00:00</td>\n      <td>Monopsony is a market structure in which there...</td>\n      <td>assistant</td>\n      <td>en</td>\n      <td>3</td>\n      <td>True</td>\n      <td>False</td>\n      <td>1.0</td>\n      <td>False</td>\n      <td>None</td>\n      <td>{'toxicity': 0.0008866374846547842, 'severe_to...</td>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>ready_for_export</td>\n      <td>{'name': ['+1', '_skip_reply', '_skip_labeling...</td>\n      <td>{'name': ['spam', 'fails_task', 'lang_mismatch...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>18145bf4-37fd-4ac0-80f5-6108b5f2b365</td>\n      <td>343ee2d4-87ae-41fd-a768-bdd65959dc4a</td>\n      <td>e10e99a0-38ac-4b07-bf5d-4427696e4e0d</td>\n      <td>2023-02-06T18:52:51.428543+00:00</td>\n      <td>How can one fight back when a monospony had be...</td>\n      <td>prompter</td>\n      <td>en</td>\n      <td>3</td>\n      <td>True</td>\n      <td>False</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>None</td>\n      <td>{'toxicity': 0.0009362137061543763, 'severe_to...</td>\n      <td>6ab24d72-0181-4594-a9cd-deaf170242fb</td>\n      <td>ready_for_export</td>\n      <td>{'name': ['+1'], 'count': [1]}</td>\n      <td>{'name': ['spam', 'lang_mismatch', 'pii', 'not...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"def PrepareData(data):\n    output = []\n    for i in range(0, len(data) - 1, 2): \n        user = data.iloc[i][\"text\"]\n        assistant = data.iloc[i + 1][\"text\"]\n        \n        output.append([\n            {\"role\": \"user\", \"content\": user},\n            {\"role\": \"assistant\", \"content\": assistant}\n        ])\n        \n    return [{\"messages\":pair} for pair in output]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:33.247173Z","iopub.execute_input":"2025-08-08T11:10:33.247495Z","iopub.status.idle":"2025-08-08T11:10:33.252001Z","shell.execute_reply.started":"2025-08-08T11:10:33.247476Z","shell.execute_reply":"2025-08-08T11:10:33.251340Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"prep_data = PrepareData(combined_instruction_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:33.252774Z","iopub.execute_input":"2025-08-08T11:10:33.253076Z","iopub.status.idle":"2025-08-08T11:10:36.702153Z","shell.execute_reply.started":"2025-08-08T11:10:33.253022Z","shell.execute_reply":"2025-08-08T11:10:36.701545Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"prep_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:36.702862Z","iopub.execute_input":"2025-08-08T11:10:36.703149Z","iopub.status.idle":"2025-08-08T11:10:36.708181Z","shell.execute_reply.started":"2025-08-08T11:10:36.703124Z","shell.execute_reply":"2025-08-08T11:10:36.707490Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"{'messages': [{'role': 'user',\n   'content': 'Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.'},\n  {'role': 'assistant',\n   'content': '\"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.'}]}"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"hugging_face_dataset = Dataset.from_list(prep_data)\n\nhugging_face_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:36.710730Z","iopub.execute_input":"2025-08-08T11:10:36.710931Z","iopub.status.idle":"2025-08-08T11:10:37.365961Z","shell.execute_reply.started":"2025-08-08T11:10:36.710916Z","shell.execute_reply":"2025-08-08T11:10:37.365369Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['messages'],\n    num_rows: 44419\n})"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"hugging_face_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:37.366590Z","iopub.execute_input":"2025-08-08T11:10:37.366795Z","iopub.status.idle":"2025-08-08T11:10:37.371286Z","shell.execute_reply.started":"2025-08-08T11:10:37.366778Z","shell.execute_reply":"2025-08-08T11:10:37.370715Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['messages'],\n    num_rows: 44419\n})"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"torch.cuda.empty_cache()\nimport gc\ngc.collect()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:37.371870Z","iopub.execute_input":"2025-08-08T11:10:37.372047Z","iopub.status.idle":"2025-08-08T11:10:38.088591Z","shell.execute_reply.started":"2025-08-08T11:10:37.372034Z","shell.execute_reply":"2025-08-08T11:10:38.087875Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"429"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=Config.model_name,\n    max_seq_length=2048,\n    dtype=None,\n    load_in_4bit=True,\n    load_in_8bit=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:38.089473Z","iopub.execute_input":"2025-08-08T11:10:38.089723Z","iopub.status.idle":"2025-08-08T11:10:51.946170Z","shell.execute_reply.started":"2025-08-08T11:10:38.089700Z","shell.execute_reply":"2025-08-08T11:10:51.945586Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.8.1: Fast Qwen3 patching. Transformers: 4.55.0. vLLM: 0.10.0.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/567M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd208566325c47ac85aadfcc40fff84b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/166 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a87465f1ce504a3a83fe769e08bf8b77"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"021d5643168c46d2a3ba5a7febab90f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92308a9878be41a08f279028ed48c7cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d4fdb19f29f4753be3ed953fb756909"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5282e9de4fed45c391b239d5b9ba0459"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/617 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63b9e2a619ec4525b8ab987a974adef8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c73f5d8528214f22b96cb61f98068221"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"list(CHAT_TEMPLATES)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:51.946868Z","iopub.execute_input":"2025-08-08T11:10:51.947159Z","iopub.status.idle":"2025-08-08T11:10:51.952257Z","shell.execute_reply.started":"2025-08-08T11:10:51.947134Z","shell.execute_reply":"2025-08-08T11:10:51.951665Z"}},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"['unsloth',\n 'zephyr',\n 'chatml',\n 'mistral',\n 'llama',\n 'vicuna',\n 'vicuna_old',\n 'vicuna old',\n 'alpaca',\n 'gemma',\n 'gemma_chatml',\n 'gemma2',\n 'gemma2_chatml',\n 'llama-3',\n 'llama3',\n 'phi-3',\n 'phi-35',\n 'phi-3.5',\n 'llama-3.1',\n 'llama-31',\n 'llama-3.2',\n 'llama-3.3',\n 'llama-32',\n 'llama-33',\n 'qwen-2.5',\n 'qwen-25',\n 'qwen25',\n 'qwen2.5',\n 'phi-4',\n 'gemma-3',\n 'gemma3',\n 'qwen-3',\n 'qwen3',\n 'gemma-3n',\n 'gemma3n']"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"tokenizer = get_chat_template(tokenizer=tokenizer, chat_template=\"qwen-3\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:51.953001Z","iopub.execute_input":"2025-08-08T11:10:51.953183Z","iopub.status.idle":"2025-08-08T11:10:51.968093Z","shell.execute_reply.started":"2025-08-08T11:10:51.953168Z","shell.execute_reply":"2025-08-08T11:10:51.967192Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"tokenizer.chat_template","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:51.968749Z","iopub.execute_input":"2025-08-08T11:10:51.969049Z","iopub.status.idle":"2025-08-08T11:10:51.991685Z","shell.execute_reply.started":"2025-08-08T11:10:51.969024Z","shell.execute_reply":"2025-08-08T11:10:51.990988Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'\\n{%- if tools %}\\n    {{- \\'<|im_start|>system\\n\\' }}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- messages[0].content + \\'\\n\\n\\' }}\\n    {%- endif %}\\n    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\\n{%- else %}\\n    {%- if messages[0].role == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\n\\' + messages[0].content + \\'<|im_end|>\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\\n{%- for forward_message in messages %}\\n    {%- set index = (messages|length - 1) - loop.index0 %}\\n    {%- set message = messages[index] %}\\n    {%- set current_content = message.content if message.content is not none else \\'\\' %}\\n    {%- set tool_start = \\'<tool_response>\\' %}\\n    {%- set tool_start_length = tool_start|length %}\\n    {%- set start_of_message = current_content[:tool_start_length] %}\\n    {%- set tool_end = \\'</tool_response>\\' %}\\n    {%- set tool_end_length = tool_end|length %}\\n    {%- set start_pos = (current_content|length) - tool_end_length %}\\n    {%- if start_pos < 0 %}\\n        {%- set start_pos = 0 %}\\n    {%- endif %}\\n    {%- set end_of_message = current_content[start_pos:] %}\\n    {%- if ns.multi_step_tool and message.role == \"user\" and not(start_of_message == tool_start and end_of_message == tool_end) %}\\n        {%- set ns.multi_step_tool = false %}\\n        {%- set ns.last_query_index = index %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {%- set content = message.content %}\\n        {%- set reasoning_content = \\'\\' %}\\n        {%- if message.reasoning_content is defined and message.reasoning_content is not none %}\\n            {%- set reasoning_content = message.reasoning_content %}\\n        {%- else %}\\n            {%- if \\'</think>\\' in message.content %}\\n                {%- set content = (message.content.split(\\'</think>\\')|last).lstrip(\\'\\n\\') %}\\n                {%- set reasoning_content = (message.content.split(\\'</think>\\')|first).rstrip(\\'\\n\\') %}\\n                {%- set reasoning_content = (reasoning_content.split(\\'<think>\\')|last).lstrip(\\'\\n\\') %}\\n            {%- endif %}\\n        {%- endif %}\\n        {%- if loop.index0 > ns.last_query_index %}\\n            {%- if loop.last or (not loop.last and reasoning_content) %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\n<think>\\n\\' + reasoning_content.strip(\\'\\n\\') + \\'\\n</think>\\n\\n\\' + content.lstrip(\\'\\n\\') }}\\n            {%- else %}\\n                {{- \\'<|im_start|>\\' + message.role + \\'\\n\\' + content }}\\n            {%- endif %}\\n        {%- else %}\\n            {{- \\'<|im_start|>\\' + message.role + \\'\\n\\' + content }}\\n        {%- endif %}\\n        {%- if message.tool_calls %}\\n            {%- for tool_call in message.tool_calls %}\\n                {%- if (loop.first and content) or (not loop.first) %}\\n                    {{- \\'\\n\\' }}\\n                {%- endif %}\\n                {%- if tool_call.function %}\\n                    {%- set tool_call = tool_call.function %}\\n                {%- endif %}\\n                {{- \\'<tool_call>\\n{\"name\": \"\\' }}\\n                {{- tool_call.name }}\\n                {{- \\'\", \"arguments\": \\' }}\\n                {%- if tool_call.arguments is string %}\\n                    {{- tool_call.arguments }}\\n                {%- else %}\\n                    {{- tool_call.arguments | tojson }}\\n                {%- endif %}\\n                {{- \\'}\\n</tool_call>\\' }}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\'<|im_end|>\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\n<tool_response>\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\n\\' }}\\n    {%- if enable_thinking is defined and enable_thinking is false %}\\n        {{- \\'<think>\\n\\n</think>\\n\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n'"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"tokenizer.chat_template =  \"\"\"{% for message in messages %}\n                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}{{ eos_token }}\n                {% endif %}\n                {% endfor %}\"\"\"\n\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:51.992394Z","iopub.execute_input":"2025-08-08T11:10:51.992600Z","iopub.status.idle":"2025-08-08T11:10:52.004207Z","shell.execute_reply.started":"2025-08-08T11:10:51.992580Z","shell.execute_reply":"2025-08-08T11:10:52.003640Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:52.004878Z","iopub.execute_input":"2025-08-08T11:10:52.005177Z","iopub.status.idle":"2025-08-08T11:10:52.018217Z","shell.execute_reply.started":"2025-08-08T11:10:52.005161Z","shell.execute_reply":"2025-08-08T11:10:52.017528Z"}},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"tokenizer.pad_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:52.019051Z","iopub.execute_input":"2025-08-08T11:10:52.019321Z","iopub.status.idle":"2025-08-08T11:10:52.032209Z","shell.execute_reply.started":"2025-08-08T11:10:52.019305Z","shell.execute_reply":"2025-08-08T11:10:52.031642Z"}},"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"tokenizer.chat_template","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:52.032838Z","iopub.execute_input":"2025-08-08T11:10:52.033091Z","iopub.status.idle":"2025-08-08T11:10:52.045965Z","shell.execute_reply.started":"2025-08-08T11:10:52.033069Z","shell.execute_reply":"2025-08-08T11:10:52.045418Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"\"{% for message in messages %}\\n                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\\n                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\\n                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}{{ eos_token }}\\n                {% endif %}\\n                {% endfor %}\""},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:52.046543Z","iopub.execute_input":"2025-08-08T11:10:52.046697Z","iopub.status.idle":"2025-08-08T11:10:52.063507Z","shell.execute_reply.started":"2025-08-08T11:10:52.046685Z","shell.execute_reply":"2025-08-08T11:10:52.062993Z"}},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"Qwen3ForCausalLM(\n  (model): Qwen3Model(\n    (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n    (layers): ModuleList(\n      (0-1): 2 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n      (2): Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n      (3-26): 24 x Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear4bit(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear4bit(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n      (27): Qwen3DecoderLayer(\n        (self_attn): Qwen3Attention(\n          (q_proj): Linear4bit(in_features=1024, out_features=2048, bias=False)\n          (k_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (v_proj): Linear4bit(in_features=1024, out_features=1024, bias=False)\n          (o_proj): Linear4bit(in_features=2048, out_features=1024, bias=False)\n          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n          (rotary_emb): LlamaRotaryEmbedding()\n        )\n        (mlp): Qwen3MLP(\n          (gate_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (up_proj): Linear(in_features=1024, out_features=3072, bias=False)\n          (down_proj): Linear(in_features=3072, out_features=1024, bias=False)\n          (act_fn): SiLU()\n        )\n        (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n    (rotary_emb): LlamaRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n)"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"qlora_model = FastLanguageModel.get_peft_model(\n    model=model, \n    r=8,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_alpha=16,\n    lora_dropout=0.0,\n    bias='none',\n    use_gradient_checkpointing='unsloth',\n    random_state=Config.random_state,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:52.064385Z","iopub.execute_input":"2025-08-08T11:10:52.064621Z","iopub.status.idle":"2025-08-08T11:10:59.237493Z","shell.execute_reply.started":"2025-08-08T11:10:52.064601Z","shell.execute_reply":"2025-08-08T11:10:59.236736Z"}},"outputs":[{"name":"stderr","text":"Unsloth 2025.8.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"def TestInstructModel(model, tokenizer, user_query, enable_thinking=False):\n    user_prompt = [\n        {\"role\":\"user\", \"content\":user_query}\n    ]\n\n    chat_template = tokenizer.apply_chat_template(user_prompt, \n                                                  tokenize=False,\n                                                  add_generation_prompt=True,\n                                                  enable_thinking=True if enable_thinking else False)\n\n    print(\"Model Inferencing...\")\n    print(\"---\"*50)\n\n    print(\"Chat Template: \", chat_template)\n\n    tokenized_template = tokenizer(chat_template, return_tensors=\"pt\", truncation=True).to(\"cuda:0\")\n\n    print(\"Bot Response:\")\n    _ = model.generate(\n                      **tokenized_template,\n                      max_new_tokens = 1024,\n                      eos_token_id=tokenizer.eos_token_id,\n                      pad_token_id=tokenizer.eos_token_id,\n                      temperature = .7 if not enable_thinking else .6, \n                      top_p = 0.8 if not enable_thinking else .95, \n                      top_k = 20,\n                      streamer = TextStreamer(tokenizer, skip_prompt=True)\n     )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:10:19.014099Z","iopub.execute_input":"2025-08-08T12:10:19.014387Z","iopub.status.idle":"2025-08-08T12:10:19.020392Z","shell.execute_reply.started":"2025-08-08T12:10:19.014365Z","shell.execute_reply":"2025-08-08T12:10:19.019653Z"}},"outputs":[],"execution_count":87},{"cell_type":"markdown","source":"#### Response Before Training","metadata":{}},{"cell_type":"code","source":"FastLanguageModel.for_inference(qlora_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:59.245212Z","iopub.execute_input":"2025-08-08T11:10:59.245401Z","iopub.status.idle":"2025-08-08T11:10:59.297740Z","shell.execute_reply.started":"2025-08-08T11:10:59.245387Z","shell.execute_reply":"2025-08-08T11:10:59.297080Z"}},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen3ForCausalLM(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n        (layers): ModuleList(\n          (0-1): 2 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (2): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (3-26): 24 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (27): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"TestInstructModel(model=qlora_model,\n                  tokenizer=tokenizer, \n                  user_query=\"Can you write a short introduction about the relevance of the term 'monopsony' in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:10:59.298456Z","iopub.execute_input":"2025-08-08T11:10:59.298636Z","iopub.status.idle":"2025-08-08T11:11:54.341458Z","shell.execute_reply.started":"2025-08-08T11:10:59.298622Z","shell.execute_reply":"2025-08-08T11:11:54.340923Z"}},"outputs":[{"name":"stdout","text":"Chat Template:  User: Can you write a short introduction about the relevance of the term 'monopsony' in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n\n\n\n\nBot Response:\nMonopsony, a term coined by economist John Commons, refers to a market structure where a single buyer has the power to purchase a significant quantity of a good or service at a price that is significantly lower than what would be possible under a competitive market. This concept is particularly relevant in the labour market, where it can manifest in various forms, such as a single employer controlling a large portion of the workforce or a single buyer controlling a significant portion of the goods being sold.\n\nIn the context of the labour market, a potential monopsony can be observed in several scenarios. For instance, in a highly competitive labour market, where multiple employers are competing for the same pool of workers, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nOn the other hand, in a highly competitive labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nIn the context of the labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nIn the context of the labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nIn the context of the labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nIn the context of the labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nIn the context of the labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nIn the context of the labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or the employer's ability to control the labour supply.\n\nIn the context of the labour market, a potential monopsony could also be observed in the context of the labour market. For example, in a highly competitive labour market, a single employer might be able to hire a large number of workers at a price that is significantly lower than what would be possible under a competitive market. This could be due to factors such as the employer's ability to negotiate better terms with the workers, the availability of skilled workers, or\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def AddChatTemplate(data_batch, tokenizer):\n    data_with_template = []\n    for data in data_batch[\"messages\"]:\n        data_with_template.append(tokenizer.apply_chat_template(data, tokenize=False, add_generation_prompt=False))\n    return {\"template_text\":data_with_template}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:11:54.342192Z","iopub.execute_input":"2025-08-08T11:11:54.342447Z","iopub.status.idle":"2025-08-08T11:11:54.346581Z","shell.execute_reply.started":"2025-08-08T11:11:54.342424Z","shell.execute_reply":"2025-08-08T11:11:54.345915Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"prep_instruction_data = hugging_face_dataset.map(lambda x: AddChatTemplate(x,tokenizer) , batched=True)\n\nprep_instruction_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:11:54.347642Z","iopub.execute_input":"2025-08-08T11:11:54.347888Z","iopub.status.idle":"2025-08-08T11:11:56.988525Z","shell.execute_reply.started":"2025-08-08T11:11:54.347864Z","shell.execute_reply":"2025-08-08T11:11:56.987836Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/44419 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"affc4a4c5a9e4f9791282f26c440163d"}},"metadata":{}},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['messages', 'template_text'],\n    num_rows: 44419\n})"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"prep_instruction_data[0][\"template_text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:11:56.992181Z","iopub.execute_input":"2025-08-08T11:11:56.992556Z","iopub.status.idle":"2025-08-08T11:11:56.997231Z","shell.execute_reply.started":"2025-08-08T11:11:56.992538Z","shell.execute_reply":"2025-08-08T11:11:56.996582Z"}},"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"'User: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\\n\\nAssistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\\n\\nRecent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\\n\\nOverall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\\n\\nReferences:\\nBivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78.<|endoftext|>\\n'"},"metadata":{}}],"execution_count":32},{"cell_type":"code","source":"# Prepare model for efficient finetuning\nFastLanguageModel.for_training(qlora_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:11:56.997850Z","iopub.execute_input":"2025-08-08T11:11:56.998042Z","iopub.status.idle":"2025-08-08T11:11:57.033951Z","shell.execute_reply.started":"2025-08-08T11:11:56.998027Z","shell.execute_reply":"2025-08-08T11:11:57.033369Z"}},"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen3ForCausalLM(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n        (layers): ModuleList(\n          (0-1): 2 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (2): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (3-26): 24 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (27): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":33},{"cell_type":"code","source":" sft_config = SFTConfig(output_dir=\"./instruction_model\", \n                       num_train_epochs=1,\n                       do_eval=True,\n                       do_train=True,\n                       per_device_train_batch_size=2,\n                       learning_rate=2e-5, \n                       weight_decay=0.01, \n                       lr_scheduler_type=\"linear\", \n                       logging_strategy=\"steps\", \n                       save_strategy=\"steps\", \n                       seed=Config.random_state, \n                       fp16=not is_bfloat16_supported(),\n                       bf16=is_bfloat16_supported(),\n                       dataset_text_field=\"template_text\",\n                       gradient_checkpointing=True,\n                       gradient_accumulation_steps=16,\n                       eos_token=tokenizer.eos_token,\n                       pad_token=tokenizer.pad_token,\n                       optim=\"adamw_8bit\",\n                       report_to=\"none\")\n\nsft_trainer = SFTTrainer(model=qlora_model,\n                         args=sft_config, \n                         train_dataset=prep_instruction_data.select(range(1000)), # Due to limited resources i reduced the data to 1000 data point \n                         eval_dataset=None, \n                         processing_class=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:11:57.034627Z","iopub.execute_input":"2025-08-08T11:11:57.034824Z","iopub.status.idle":"2025-08-08T11:11:58.999832Z","shell.execute_reply.started":"2025-08-08T11:11:57.034807Z","shell.execute_reply":"2025-08-08T11:11:58.999205Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"template_text\"] (num_proc=2):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7367bf2e31240dfae82575eeee728c0"}},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"trainind_stata = sft_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:11:59.000789Z","iopub.execute_input":"2025-08-08T11:11:59.001430Z","iopub.status.idle":"2025-08-08T11:17:31.202608Z","shell.execute_reply.started":"2025-08-08T11:11:59.001392Z","shell.execute_reply":"2025-08-08T11:17:31.202003Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 1,000 | Num Epochs = 1 | Total steps = 32\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 16\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 16 x 1) = 32\n \"-____-\"     Trainable parameters = 5,046,272 of 601,096,192 (0.84% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [32/32 05:18, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.188400</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.844400</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>1.846000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>2.027900</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>1.789000</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.815600</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.946000</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.008600</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.301400</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.868400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.948200</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.990600</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.228100</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.979800</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.996200</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>2.024100</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>2.223800</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>1.935000</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>1.721100</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.969800</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>2.011400</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>2.258000</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>1.763600</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>2.090800</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>1.696100</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>1.955400</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>2.526800</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>1.952500</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>2.261200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>2.055800</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>2.038500</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>1.745000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# Prepare model for inferencing\nFastLanguageModel.for_inference(sft_trainer.model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:17:31.203395Z","iopub.execute_input":"2025-08-08T11:17:31.203647Z","iopub.status.idle":"2025-08-08T11:17:31.225318Z","shell.execute_reply.started":"2025-08-08T11:17:31.203625Z","shell.execute_reply":"2025-08-08T11:17:31.224686Z"}},"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen3ForCausalLM(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n        (layers): ModuleList(\n          (0-1): 2 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (2): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (3-26): 24 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (27): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":36},{"cell_type":"markdown","source":"#### Response After Training","metadata":{}},{"cell_type":"code","source":"TestInstructModel(model=sft_trainer.model,\n                  tokenizer=sft_trainer.tokenizer, \n                  user_query=\"Can you write a short introduction about the relevance of the term 'monopsony' in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:17:31.226293Z","iopub.execute_input":"2025-08-08T11:17:31.226555Z","iopub.status.idle":"2025-08-08T11:18:00.808497Z","shell.execute_reply.started":"2025-08-08T11:17:31.226529Z","shell.execute_reply":"2025-08-08T11:18:00.807884Z"}},"outputs":[{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"name":"stdout","text":"Chat Template:  User: Can you write a short introduction about the relevance of the term 'monopsony' in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.\n\n\n\n\nBot Response:\nMonopsony is a term used in economics to describe a market structure where a single buyer has the power to purchase a large quantity of a good or service at a price that is significantly lower than the price that would prevail in a competitive market. This can occur in situations where there is a large number of buyers and a small number of sellers, or where there is a large number of sellers and a small number of buyers. In such a market, the single buyer can influence the price of the good or service, and the price can be set below the competitive level.\n\nOne example of a potential monopsony in the labour market is the case of a single employer who hires a large number of workers. In this case, the employer has the power to hire a large number of workers at a price that is significantly lower than the price that would prevail in a competitive market. This can occur if the employer has a large number of workers and a small number of employers, or if the employer has a large number of workers and a small number of employers. In such a market, the single employer can influence the price of the labour, and the price can be set below the competitive level.\n\nAnother example of a potential monopsony in the labour market is the case of a single employer who hires a large number of workers and is able to negotiate a lower wage rate with the workers. In this case, the single employer has the power to negotiate a lower wage rate with the workers, and the wage rate can be set below the competitive level. This can occur if the single employer has a large number of workers and a small number of employers, or if the single employer has a large number of workers and a small number of employers.\n\nResearch has shown that monopsonies can have significant effects on the economy. Monopsonies can lead to lower wages and prices, which can have negative effects on the economy. Monopsonies can also lead to inefficiencies in the economy, as the single buyer may not be able to purchase the goods or services that the other buyers would be able to purchase. This can lead to a reduction in the overall efficiency of the economy.\n\nIn conclusion, the term 'monopsony' is a term used in economics to describe a market structure where a single buyer has the power to purchase a large quantity of a good or service at a price that is significantly lower than the price that would prevail in a competitive market. Monopsonies can occur in situations where there is a large number of buyers and a small number of sellers, or where there is a large number of sellers and a small number of buyers. Monopsonies can have significant effects on the economy, and they can lead to lower wages and prices, inefficiencies, and reduced overall efficiency of the economy.<|endoftext|>\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"dpo_data = load_dataset(Config.dpo_dataset_name)\n\ndpo_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:53:23.797887Z","iopub.execute_input":"2025-08-08T11:53:23.798217Z","iopub.status.idle":"2025-08-08T11:53:24.940071Z","shell.execute_reply.started":"2025-08-08T11:53:23.798194Z","shell.execute_reply":"2025-08-08T11:53:24.939296Z"}},"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train_prefs: Dataset({\n        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n        num_rows: 61135\n    })\n    train_sft: Dataset({\n        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n        num_rows: 61135\n    })\n    test_prefs: Dataset({\n        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n        num_rows: 2000\n    })\n    test_sft: Dataset({\n        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n        num_rows: 1000\n    })\n    train_gen: Dataset({\n        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n        num_rows: 61135\n    })\n    test_gen: Dataset({\n        features: ['prompt', 'prompt_id', 'chosen', 'rejected', 'messages', 'score_chosen', 'score_rejected'],\n        num_rows: 1000\n    })\n})"},"metadata":{}}],"execution_count":67},{"cell_type":"code","source":"dpo_df = dpo_data[\"train_prefs\"].to_pandas()\n\ndpo_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:53:24.941371Z","iopub.execute_input":"2025-08-08T11:53:24.941593Z","iopub.status.idle":"2025-08-08T11:53:26.081994Z","shell.execute_reply.started":"2025-08-08T11:53:24.941574Z","shell.execute_reply":"2025-08-08T11:53:26.081251Z"}},"outputs":[{"execution_count":68,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0         how can i develop a habit of drawing daily   \n1  how can I transform the getPosition method of ...   \n2  Given a sentence in French, provide an equival...   \n3      Which animal has two hands, a hyrax or a dog?   \n4  Can you explain more about how Tamping Coke is...   \n\n                                           prompt_id  \\\n0  086b3e24f29b8956a01059f79c56db35d118a06fb6b844...   \n1  2766cbd1fed7f982d94b031596e771c841668bd8913839...   \n2  0efb42706b3fcc906f579505c7cc0c4e68a640ab3862b1...   \n3  d169f4610d69b94f54b6923e11aeffe2e321e272395686...   \n4  ac205b0f69d45eae8dbf446673bd78284e230b6ce3381e...   \n\n                                              chosen  \\\n0  [{'content': 'how can i develop a habit of dra...   \n1  [{'content': 'how can I transform the getPosit...   \n2  [{'content': 'Given a sentence in French, prov...   \n3  [{'content': 'Which animal has two hands, a hy...   \n4  [{'content': 'Can you explain more about how T...   \n\n                                            rejected  \\\n0  [{'content': 'how can i develop a habit of dra...   \n1  [{'content': 'how can I transform the getPosit...   \n2  [{'content': 'Given a sentence in French, prov...   \n3  [{'content': 'Which animal has two hands, a hy...   \n4  [{'content': 'Can you explain more about how T...   \n\n                                            messages  score_chosen  \\\n0  [{'content': 'how can i develop a habit of dra...           8.5   \n1  [{'content': 'how can I transform the getPosit...           6.5   \n2  [{'content': 'Given a sentence in French, prov...           6.0   \n3  [{'content': 'Which animal has two hands, a hy...           9.0   \n4  [{'content': 'Can you explain more about how T...           8.0   \n\n   score_rejected  \n0             8.5  \n1             6.5  \n2             3.0  \n3             8.0  \n4             6.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>prompt_id</th>\n      <th>chosen</th>\n      <th>rejected</th>\n      <th>messages</th>\n      <th>score_chosen</th>\n      <th>score_rejected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>how can i develop a habit of drawing daily</td>\n      <td>086b3e24f29b8956a01059f79c56db35d118a06fb6b844...</td>\n      <td>[{'content': 'how can i develop a habit of dra...</td>\n      <td>[{'content': 'how can i develop a habit of dra...</td>\n      <td>[{'content': 'how can i develop a habit of dra...</td>\n      <td>8.5</td>\n      <td>8.5</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>how can I transform the getPosition method of ...</td>\n      <td>2766cbd1fed7f982d94b031596e771c841668bd8913839...</td>\n      <td>[{'content': 'how can I transform the getPosit...</td>\n      <td>[{'content': 'how can I transform the getPosit...</td>\n      <td>[{'content': 'how can I transform the getPosit...</td>\n      <td>6.5</td>\n      <td>6.5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Given a sentence in French, provide an equival...</td>\n      <td>0efb42706b3fcc906f579505c7cc0c4e68a640ab3862b1...</td>\n      <td>[{'content': 'Given a sentence in French, prov...</td>\n      <td>[{'content': 'Given a sentence in French, prov...</td>\n      <td>[{'content': 'Given a sentence in French, prov...</td>\n      <td>6.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Which animal has two hands, a hyrax or a dog?</td>\n      <td>d169f4610d69b94f54b6923e11aeffe2e321e272395686...</td>\n      <td>[{'content': 'Which animal has two hands, a hy...</td>\n      <td>[{'content': 'Which animal has two hands, a hy...</td>\n      <td>[{'content': 'Which animal has two hands, a hy...</td>\n      <td>9.0</td>\n      <td>8.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Can you explain more about how Tamping Coke is...</td>\n      <td>ac205b0f69d45eae8dbf446673bd78284e230b6ce3381e...</td>\n      <td>[{'content': 'Can you explain more about how T...</td>\n      <td>[{'content': 'Can you explain more about how T...</td>\n      <td>[{'content': 'Can you explain more about how T...</td>\n      <td>8.0</td>\n      <td>6.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":68},{"cell_type":"code","source":"dpo_model, dpo_tokenizer = FastLanguageModel.from_pretrained(\n    model_name=Config.model_name,\n    max_seq_length=2048,\n    dtype=None,\n    load_in_4bit=True,\n)\n\n# Apply the LoRA weights from checkpoint\ndpo_model = PeftModel.from_pretrained(\n    model, \n    \"/kaggle/working/instruction_model/checkpoint-32\",\n    is_trainable=True\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T11:53:26.083578Z","iopub.execute_input":"2025-08-08T11:53:26.083819Z","iopub.status.idle":"2025-08-08T11:53:33.161142Z","shell.execute_reply.started":"2025-08-08T11:53:26.083801Z","shell.execute_reply":"2025-08-08T11:53:33.160306Z"}},"outputs":[{"name":"stdout","text":"==((====))==  Unsloth 2025.8.1: Fast Qwen3 patching. Transformers: 4.55.0. vLLM: 0.10.0.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 6.0. CUDA Toolkit: 12.6. Triton: 3.3.1\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.31. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"dpo_tokenizer.chat_template =  \"\"\"{% for message in messages %}\n                {% if message['role'] == 'system' %}System: {{ message['content'] }}\\n\n                {% elif message['role'] == 'user' %}User: {{ message['content'] }}\\n\n                {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}{{ eos_token }}\n                {% endif %}\n                {% endfor %}\"\"\"\n\ndpo_tokenizer.pad_token = dpo_tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:26:59.726237Z","iopub.execute_input":"2025-08-08T12:26:59.726729Z","iopub.status.idle":"2025-08-08T12:26:59.730437Z","shell.execute_reply.started":"2025-08-08T12:26:59.726706Z","shell.execute_reply":"2025-08-08T12:26:59.729735Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"def PrepareDpoData(df, tokenizer):\n    data_list = []\n    for idx, rows in tqdm.tqdm(df.iterrows(), total=df.shape[0]):\n        \n        data_list.append(\n        {\n        \"prompt\": tokenizer.apply_chat_template([{\"role\":\"user\", \"content\":rows[\"messages\"][0][\"content\"]}], tokenize=False),\n        \"chosen\": tokenizer.apply_chat_template([{\"role\":\"assistant\", \"content\":rows[\"chosen\"][1][\"content\"]}], tokenize=False),\n        \"rejected\": tokenizer.apply_chat_template([{\"role\":\"assistant\", \"content\":rows[\"rejected\"][1][\"content\"]}], tokenize=False)\n        }\n    )\n    return Dataset.from_list(data_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:59:08.946123Z","iopub.execute_input":"2025-08-08T12:59:08.946765Z","iopub.status.idle":"2025-08-08T12:59:08.953391Z","shell.execute_reply.started":"2025-08-08T12:59:08.946735Z","shell.execute_reply":"2025-08-08T12:59:08.952469Z"}},"outputs":[],"execution_count":112},{"cell_type":"code","source":"dpo_formatted_data = PrepareDpoData(dpo_df, dpo_tokenizer)\n\ndpo_formatted_data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:59:11.166512Z","iopub.execute_input":"2025-08-08T12:59:11.167198Z","iopub.status.idle":"2025-08-08T12:59:23.684644Z","shell.execute_reply.started":"2025-08-08T12:59:11.167163Z","shell.execute_reply":"2025-08-08T12:59:23.683867Z"}},"outputs":[{"name":"stderr","text":"100%|| 61135/61135 [00:10<00:00, 5765.68it/s]\n","output_type":"stream"},{"execution_count":113,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['prompt', 'chosen', 'rejected'],\n    num_rows: 61135\n})"},"metadata":{}}],"execution_count":113},{"cell_type":"code","source":"dpo_formatted_data[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:59:23.686044Z","iopub.execute_input":"2025-08-08T12:59:23.686312Z","iopub.status.idle":"2025-08-08T12:59:23.691288Z","shell.execute_reply.started":"2025-08-08T12:59:23.686293Z","shell.execute_reply":"2025-08-08T12:59:23.690552Z"}},"outputs":[{"execution_count":114,"output_type":"execute_result","data":{"text/plain":"{'prompt': 'User: how can i develop a habit of drawing daily\\n\\n',\n 'chosen': \"Assistant: Developing a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\\n\\n1. Set a specific time: Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\\n2. Set a specific duration: Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\\n3. Start small and simple: Don't try to create a masterpiece every day, start with simple and easy-to-do sketches. Focus on improving your skills gradually.\\n4. Use a variety of tools and mediums: Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\\n5. Take breaks and rest: Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\\n6. Challenge yourself: Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\\n7. Track your progress: Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\\n\\nRemember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.<|endoftext|>\\n\",\n 'rejected': \"Assistant: As an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\\n\\n1. Start small: Start with simple drawings or doodles and gradually increase the complexity of your drawings.\\n\\n2. Set a schedule: Set a specific time every day to draw, and try to stick to it as much as possible.\\n\\n3. Make it fun: Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\\n\\n4. Use resources: There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\\n\\n5. Surround yourself with inspiration: Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\\n\\nRemember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.<|endoftext|>\\n\"}"},"metadata":{}}],"execution_count":114},{"cell_type":"code","source":"FastLanguageModel.for_training(dpo_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:59:37.354999Z","iopub.execute_input":"2025-08-08T12:59:37.355783Z","iopub.status.idle":"2025-08-08T12:59:37.399335Z","shell.execute_reply.started":"2025-08-08T12:59:37.355749Z","shell.execute_reply":"2025-08-08T12:59:37.398667Z"}},"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen3ForCausalLM(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n        (layers): ModuleList(\n          (0-1): 2 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (2): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (3-26): 24 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (27): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":115},{"cell_type":"code","source":"PatchDPOTrainer() # Add this for efficient finetuning\n\ndpo_config = DPOConfig(output_dir=\"dpo_model_from_instruction_model\",\n                       num_train_epochs=1,\n                       do_train=True, \n                       do_eval=False,\n                       per_device_train_batch_size=2,\n                       learning_rate=2e-5,\n                       beta=.2,\n                       weight_decay=0.01, \n                       lr_scheduler_type=\"linear\", \n                       logging_strategy=\"steps\",\n                       save_strategy=\"steps\", \n                       seed=Config.random_state, \n                       fp16=not is_bfloat16_supported(),\n                       bf16=is_bfloat16_supported(),\n                       gradient_checkpointing=True,\n                       gradient_accumulation_steps=3,\n                       optim=\"adamw_8bit\",\n                       report_to=\"none\")\n\ndpo_trainer = DPOTrainer(model=dpo_model, \n                         ref_model=None,\n                         args=dpo_config,\n                         train_dataset=dpo_formatted_data.select(range(500)), # Due to limited resources i reduced the data to 500 data point\n                         # eval_dataset=dpo_formatted_data.shuffle(seed=Config.random_state).select(range(500)),\n                         processing_class=dpo_tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:59:42.695833Z","iopub.execute_input":"2025-08-08T12:59:42.696449Z","iopub.status.idle":"2025-08-08T12:59:54.925367Z","shell.execute_reply.started":"2025-08-08T12:59:42.696416Z","shell.execute_reply":"2025-08-08T12:59:54.924527Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Extracting prompt in train dataset (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f207fe463274dae85c98b793f1da6e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47b2432f9c5c4874a532a817b2e0c7c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset (num_proc=2):   0%|          | 0/500 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c686e3e67fb54c52b8746ff86c4e01d3"}},"metadata":{}}],"execution_count":116},{"cell_type":"code","source":"dpo_training_stats = dpo_trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T12:59:54.927129Z","iopub.execute_input":"2025-08-08T12:59:54.928051Z","iopub.status.idle":"2025-08-08T13:09:22.498790Z","shell.execute_reply.started":"2025-08-08T12:59:54.928023Z","shell.execute_reply":"2025-08-08T13:09:22.498086Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 500 | Num Epochs = 1 | Total steps = 84\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 3\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 3 x 1) = 6\n \"-____-\"     Trainable parameters = 5,046,272 of 601,096,192 (0.84% trained)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='84' max='84' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [84/84 09:19, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>rewards / chosen</th>\n      <th>rewards / rejected</th>\n      <th>rewards / accuracies</th>\n      <th>rewards / margins</th>\n      <th>logps / chosen</th>\n      <th>logps / rejected</th>\n      <th>logits / chosen</th>\n      <th>logits / rejected</th>\n      <th>eval_logits / chosen</th>\n      <th>eval_logits / rejected</th>\n      <th>nll_loss</th>\n      <th>aux_loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.114600</td>\n      <td>2.418448</td>\n      <td>0.062440</td>\n      <td>1.000000</td>\n      <td>2.356009</td>\n      <td>-94.584328</td>\n      <td>-220.605103</td>\n      <td>-1.182632</td>\n      <td>0.136104</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.249000</td>\n      <td>3.040781</td>\n      <td>0.831871</td>\n      <td>0.833333</td>\n      <td>2.208910</td>\n      <td>-178.307983</td>\n      <td>-144.500107</td>\n      <td>-0.291362</td>\n      <td>-0.476682</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.005500</td>\n      <td>4.325304</td>\n      <td>-0.965668</td>\n      <td>1.000000</td>\n      <td>5.290971</td>\n      <td>-524.623352</td>\n      <td>-423.387054</td>\n      <td>-0.932113</td>\n      <td>-0.518924</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.126200</td>\n      <td>3.535020</td>\n      <td>0.396612</td>\n      <td>1.000000</td>\n      <td>3.138408</td>\n      <td>-167.497162</td>\n      <td>-189.217834</td>\n      <td>-0.716980</td>\n      <td>-0.304245</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.158700</td>\n      <td>3.143517</td>\n      <td>0.434337</td>\n      <td>0.833333</td>\n      <td>2.709180</td>\n      <td>-185.999008</td>\n      <td>-250.433838</td>\n      <td>-0.458511</td>\n      <td>-0.822143</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.089900</td>\n      <td>3.440002</td>\n      <td>-0.627796</td>\n      <td>1.000000</td>\n      <td>4.067798</td>\n      <td>-349.794220</td>\n      <td>-415.872284</td>\n      <td>-0.174519</td>\n      <td>-0.551759</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.073600</td>\n      <td>3.422282</td>\n      <td>-0.075139</td>\n      <td>1.000000</td>\n      <td>3.497422</td>\n      <td>-466.188812</td>\n      <td>-482.306854</td>\n      <td>-0.747264</td>\n      <td>-0.563671</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>0.093000</td>\n      <td>3.505237</td>\n      <td>0.610544</td>\n      <td>1.000000</td>\n      <td>2.894693</td>\n      <td>-212.612564</td>\n      <td>-203.753540</td>\n      <td>-0.404231</td>\n      <td>-0.744077</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>0.083300</td>\n      <td>4.639887</td>\n      <td>0.780694</td>\n      <td>1.000000</td>\n      <td>3.859192</td>\n      <td>-801.399902</td>\n      <td>-573.929688</td>\n      <td>-0.510913</td>\n      <td>-0.508544</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>0.099200</td>\n      <td>3.062364</td>\n      <td>-0.112117</td>\n      <td>1.000000</td>\n      <td>3.174481</td>\n      <td>-309.558258</td>\n      <td>-307.858673</td>\n      <td>-0.729365</td>\n      <td>-0.189225</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>0.096000</td>\n      <td>3.136062</td>\n      <td>-0.270173</td>\n      <td>1.000000</td>\n      <td>3.406235</td>\n      <td>-438.372162</td>\n      <td>-390.433563</td>\n      <td>-0.376551</td>\n      <td>-0.294985</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>0.137000</td>\n      <td>3.404732</td>\n      <td>0.107868</td>\n      <td>1.000000</td>\n      <td>3.296863</td>\n      <td>-392.843353</td>\n      <td>-228.820999</td>\n      <td>-0.367144</td>\n      <td>0.155042</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>0.039000</td>\n      <td>3.182962</td>\n      <td>-0.359027</td>\n      <td>1.000000</td>\n      <td>3.541989</td>\n      <td>-431.727661</td>\n      <td>-327.585419</td>\n      <td>-0.478809</td>\n      <td>0.232432</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>0.034000</td>\n      <td>2.953586</td>\n      <td>-1.050901</td>\n      <td>1.000000</td>\n      <td>4.004488</td>\n      <td>-458.627045</td>\n      <td>-728.139648</td>\n      <td>-0.520176</td>\n      <td>0.144018</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>0.080100</td>\n      <td>2.540363</td>\n      <td>-0.798245</td>\n      <td>1.000000</td>\n      <td>3.338608</td>\n      <td>-338.447113</td>\n      <td>-402.543304</td>\n      <td>-0.559867</td>\n      <td>-0.535963</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>0.198500</td>\n      <td>2.374570</td>\n      <td>-0.824501</td>\n      <td>0.833333</td>\n      <td>3.199072</td>\n      <td>-298.233917</td>\n      <td>-542.261536</td>\n      <td>-0.020714</td>\n      <td>0.173352</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>0.109200</td>\n      <td>2.260869</td>\n      <td>-0.605977</td>\n      <td>1.000000</td>\n      <td>2.866845</td>\n      <td>-293.470123</td>\n      <td>-370.538971</td>\n      <td>0.285100</td>\n      <td>0.611239</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>0.069100</td>\n      <td>2.508269</td>\n      <td>-0.916419</td>\n      <td>1.000000</td>\n      <td>3.424688</td>\n      <td>-511.980499</td>\n      <td>-432.300049</td>\n      <td>-0.014761</td>\n      <td>-0.192032</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>0.047300</td>\n      <td>2.894018</td>\n      <td>-0.401798</td>\n      <td>1.000000</td>\n      <td>3.295816</td>\n      <td>-255.400940</td>\n      <td>-331.212555</td>\n      <td>-1.698306</td>\n      <td>-0.015914</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>0.253800</td>\n      <td>2.725499</td>\n      <td>0.734584</td>\n      <td>0.833333</td>\n      <td>1.990915</td>\n      <td>-291.156647</td>\n      <td>-226.100143</td>\n      <td>-0.697218</td>\n      <td>-0.531092</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>0.176600</td>\n      <td>2.847245</td>\n      <td>0.137018</td>\n      <td>1.000000</td>\n      <td>2.710227</td>\n      <td>-403.131744</td>\n      <td>-423.785736</td>\n      <td>-0.838412</td>\n      <td>-0.327355</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>0.215000</td>\n      <td>2.046261</td>\n      <td>-0.735059</td>\n      <td>1.000000</td>\n      <td>2.781319</td>\n      <td>-318.422241</td>\n      <td>-354.529175</td>\n      <td>-0.495837</td>\n      <td>-0.915909</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>23</td>\n      <td>0.095600</td>\n      <td>1.841443</td>\n      <td>-0.796852</td>\n      <td>1.000000</td>\n      <td>2.638294</td>\n      <td>-214.779861</td>\n      <td>-321.384308</td>\n      <td>-0.151334</td>\n      <td>0.516187</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>0.274700</td>\n      <td>1.828236</td>\n      <td>-0.036135</td>\n      <td>1.000000</td>\n      <td>1.864371</td>\n      <td>-363.950531</td>\n      <td>-275.837494</td>\n      <td>-0.580251</td>\n      <td>-0.433713</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>0.149600</td>\n      <td>2.818180</td>\n      <td>-0.273016</td>\n      <td>1.000000</td>\n      <td>3.091195</td>\n      <td>-526.478821</td>\n      <td>-370.104706</td>\n      <td>-0.111677</td>\n      <td>-0.308819</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>26</td>\n      <td>0.183700</td>\n      <td>1.792810</td>\n      <td>-0.485747</td>\n      <td>1.000000</td>\n      <td>2.278558</td>\n      <td>-248.848953</td>\n      <td>-317.119080</td>\n      <td>-1.275658</td>\n      <td>-0.515286</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>27</td>\n      <td>0.207800</td>\n      <td>2.227806</td>\n      <td>-0.014005</td>\n      <td>0.833333</td>\n      <td>2.241811</td>\n      <td>-178.440048</td>\n      <td>-311.988220</td>\n      <td>-0.639677</td>\n      <td>-0.328541</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>28</td>\n      <td>0.063500</td>\n      <td>2.514378</td>\n      <td>-0.586542</td>\n      <td>1.000000</td>\n      <td>3.100921</td>\n      <td>-408.411987</td>\n      <td>-329.643768</td>\n      <td>-0.991906</td>\n      <td>-0.658827</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>29</td>\n      <td>0.542100</td>\n      <td>1.623661</td>\n      <td>0.209577</td>\n      <td>0.833333</td>\n      <td>1.414084</td>\n      <td>-290.879608</td>\n      <td>-337.991425</td>\n      <td>0.191399</td>\n      <td>-1.570311</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>0.078800</td>\n      <td>2.229866</td>\n      <td>-0.475493</td>\n      <td>1.000000</td>\n      <td>2.705359</td>\n      <td>-531.384705</td>\n      <td>-388.629974</td>\n      <td>-0.358552</td>\n      <td>-0.296537</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>31</td>\n      <td>0.075300</td>\n      <td>1.732698</td>\n      <td>-1.910419</td>\n      <td>1.000000</td>\n      <td>3.643117</td>\n      <td>-457.070679</td>\n      <td>-477.537872</td>\n      <td>-1.017378</td>\n      <td>-0.845876</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>0.347900</td>\n      <td>1.749632</td>\n      <td>0.533918</td>\n      <td>1.000000</td>\n      <td>1.215714</td>\n      <td>-288.938690</td>\n      <td>-318.464874</td>\n      <td>-0.473471</td>\n      <td>-0.773828</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>33</td>\n      <td>0.296600</td>\n      <td>1.128321</td>\n      <td>-0.287035</td>\n      <td>1.000000</td>\n      <td>1.415356</td>\n      <td>-360.114655</td>\n      <td>-271.143921</td>\n      <td>-0.721969</td>\n      <td>-0.833060</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>34</td>\n      <td>0.184100</td>\n      <td>2.620430</td>\n      <td>-0.376498</td>\n      <td>0.833333</td>\n      <td>2.996927</td>\n      <td>-464.222504</td>\n      <td>-335.786377</td>\n      <td>0.003031</td>\n      <td>-0.524727</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>0.327200</td>\n      <td>1.401588</td>\n      <td>0.175601</td>\n      <td>0.833333</td>\n      <td>1.225986</td>\n      <td>-394.103119</td>\n      <td>-295.463409</td>\n      <td>-0.540981</td>\n      <td>-0.512856</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>36</td>\n      <td>0.229900</td>\n      <td>2.501938</td>\n      <td>0.025307</td>\n      <td>1.000000</td>\n      <td>2.476631</td>\n      <td>-464.136719</td>\n      <td>-561.077881</td>\n      <td>-0.012445</td>\n      <td>-0.146743</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>37</td>\n      <td>0.093700</td>\n      <td>1.885282</td>\n      <td>-0.993771</td>\n      <td>1.000000</td>\n      <td>2.879053</td>\n      <td>-567.956726</td>\n      <td>-558.288330</td>\n      <td>-0.657204</td>\n      <td>-0.692928</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>38</td>\n      <td>0.162400</td>\n      <td>2.616957</td>\n      <td>0.326963</td>\n      <td>1.000000</td>\n      <td>2.289994</td>\n      <td>-542.629883</td>\n      <td>-290.922333</td>\n      <td>-0.626548</td>\n      <td>-0.931528</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>39</td>\n      <td>0.217100</td>\n      <td>1.375899</td>\n      <td>-1.081356</td>\n      <td>0.833333</td>\n      <td>2.457254</td>\n      <td>-175.571487</td>\n      <td>-259.152222</td>\n      <td>-1.121626</td>\n      <td>-0.204669</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>0.190700</td>\n      <td>1.504493</td>\n      <td>-0.579963</td>\n      <td>1.000000</td>\n      <td>2.084456</td>\n      <td>-337.138580</td>\n      <td>-622.154480</td>\n      <td>-0.543219</td>\n      <td>-0.296127</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>41</td>\n      <td>0.305600</td>\n      <td>1.936695</td>\n      <td>0.347979</td>\n      <td>1.000000</td>\n      <td>1.588715</td>\n      <td>-368.493256</td>\n      <td>-284.800842</td>\n      <td>-0.911364</td>\n      <td>-0.817739</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>42</td>\n      <td>0.175500</td>\n      <td>1.634061</td>\n      <td>-0.383883</td>\n      <td>1.000000</td>\n      <td>2.017944</td>\n      <td>-450.758911</td>\n      <td>-580.624756</td>\n      <td>-0.491170</td>\n      <td>-0.664340</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>43</td>\n      <td>0.360300</td>\n      <td>0.982641</td>\n      <td>-0.021673</td>\n      <td>1.000000</td>\n      <td>1.004314</td>\n      <td>-121.520782</td>\n      <td>-162.627670</td>\n      <td>-0.395419</td>\n      <td>-0.029943</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>44</td>\n      <td>0.375000</td>\n      <td>1.352726</td>\n      <td>-0.061850</td>\n      <td>0.666667</td>\n      <td>1.414576</td>\n      <td>-264.355347</td>\n      <td>-137.132370</td>\n      <td>-0.620070</td>\n      <td>-0.458293</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>0.474100</td>\n      <td>0.791512</td>\n      <td>-0.251356</td>\n      <td>0.833333</td>\n      <td>1.042868</td>\n      <td>-151.244934</td>\n      <td>-391.007324</td>\n      <td>-0.875003</td>\n      <td>0.356242</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>46</td>\n      <td>0.291300</td>\n      <td>1.323187</td>\n      <td>-0.133378</td>\n      <td>1.000000</td>\n      <td>1.456564</td>\n      <td>-245.044235</td>\n      <td>-273.239441</td>\n      <td>-0.978894</td>\n      <td>-0.839957</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>47</td>\n      <td>0.426000</td>\n      <td>1.534776</td>\n      <td>0.163083</td>\n      <td>0.833333</td>\n      <td>1.371693</td>\n      <td>-493.818451</td>\n      <td>-239.008423</td>\n      <td>0.029693</td>\n      <td>-1.384412</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>0.666100</td>\n      <td>0.658825</td>\n      <td>0.342234</td>\n      <td>0.666667</td>\n      <td>0.316591</td>\n      <td>-355.592377</td>\n      <td>-342.381134</td>\n      <td>-1.071253</td>\n      <td>-0.466007</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>49</td>\n      <td>0.387700</td>\n      <td>1.128616</td>\n      <td>0.160895</td>\n      <td>0.833333</td>\n      <td>0.967721</td>\n      <td>-334.083893</td>\n      <td>-356.189423</td>\n      <td>-0.948958</td>\n      <td>-0.328384</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>0.168200</td>\n      <td>2.059438</td>\n      <td>-0.230656</td>\n      <td>1.000000</td>\n      <td>2.290094</td>\n      <td>-553.066345</td>\n      <td>-310.323181</td>\n      <td>-0.279501</td>\n      <td>-0.311847</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>51</td>\n      <td>0.191300</td>\n      <td>1.723681</td>\n      <td>-0.174870</td>\n      <td>1.000000</td>\n      <td>1.898551</td>\n      <td>-297.444000</td>\n      <td>-349.974243</td>\n      <td>0.164060</td>\n      <td>0.342579</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>52</td>\n      <td>0.294200</td>\n      <td>1.144109</td>\n      <td>-0.145873</td>\n      <td>1.000000</td>\n      <td>1.289982</td>\n      <td>-401.094238</td>\n      <td>-366.499146</td>\n      <td>-0.745357</td>\n      <td>-0.786886</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>53</td>\n      <td>0.548500</td>\n      <td>1.281461</td>\n      <td>0.665186</td>\n      <td>0.833333</td>\n      <td>0.616276</td>\n      <td>-336.926453</td>\n      <td>-410.062256</td>\n      <td>-0.898125</td>\n      <td>-0.575941</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>54</td>\n      <td>0.208500</td>\n      <td>2.075505</td>\n      <td>0.021674</td>\n      <td>1.000000</td>\n      <td>2.053831</td>\n      <td>-402.852081</td>\n      <td>-343.319733</td>\n      <td>-0.044417</td>\n      <td>-0.024891</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>0.266900</td>\n      <td>1.520232</td>\n      <td>-0.157790</td>\n      <td>1.000000</td>\n      <td>1.678022</td>\n      <td>-411.498749</td>\n      <td>-316.813507</td>\n      <td>-0.564157</td>\n      <td>-0.609062</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>0.489300</td>\n      <td>1.196487</td>\n      <td>0.634651</td>\n      <td>0.833333</td>\n      <td>0.561837</td>\n      <td>-729.007507</td>\n      <td>-359.466461</td>\n      <td>0.659579</td>\n      <td>0.678558</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>57</td>\n      <td>0.336400</td>\n      <td>1.930390</td>\n      <td>0.760348</td>\n      <td>1.000000</td>\n      <td>1.170043</td>\n      <td>-357.147614</td>\n      <td>-277.254639</td>\n      <td>-0.776939</td>\n      <td>-0.118865</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>58</td>\n      <td>0.287600</td>\n      <td>2.476386</td>\n      <td>0.571075</td>\n      <td>0.833333</td>\n      <td>1.905311</td>\n      <td>-612.245117</td>\n      <td>-462.081635</td>\n      <td>-0.424618</td>\n      <td>-0.144332</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>59</td>\n      <td>0.453800</td>\n      <td>1.083399</td>\n      <td>0.335378</td>\n      <td>0.833333</td>\n      <td>0.748021</td>\n      <td>-336.628113</td>\n      <td>-392.579834</td>\n      <td>-0.508676</td>\n      <td>-0.166079</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>0.642200</td>\n      <td>0.810572</td>\n      <td>0.378673</td>\n      <td>0.666667</td>\n      <td>0.431899</td>\n      <td>-246.926147</td>\n      <td>-367.997162</td>\n      <td>-1.081476</td>\n      <td>-0.392414</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>61</td>\n      <td>0.630000</td>\n      <td>0.861280</td>\n      <td>0.510871</td>\n      <td>0.666667</td>\n      <td>0.350409</td>\n      <td>-382.611816</td>\n      <td>-352.904755</td>\n      <td>-0.210326</td>\n      <td>-0.421575</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>62</td>\n      <td>0.511900</td>\n      <td>1.249095</td>\n      <td>0.358371</td>\n      <td>0.666667</td>\n      <td>0.890725</td>\n      <td>-224.432175</td>\n      <td>-227.650528</td>\n      <td>-0.355907</td>\n      <td>-0.490439</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>63</td>\n      <td>1.297300</td>\n      <td>-0.301061</td>\n      <td>0.477283</td>\n      <td>0.166667</td>\n      <td>-0.778344</td>\n      <td>-270.617950</td>\n      <td>-207.648071</td>\n      <td>-0.812497</td>\n      <td>-0.310925</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>0.463800</td>\n      <td>0.989831</td>\n      <td>0.098514</td>\n      <td>0.833333</td>\n      <td>0.891317</td>\n      <td>-200.246719</td>\n      <td>-234.221359</td>\n      <td>-1.165866</td>\n      <td>-1.163067</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>0.722900</td>\n      <td>0.981143</td>\n      <td>-0.193478</td>\n      <td>0.500000</td>\n      <td>1.174621</td>\n      <td>-445.937744</td>\n      <td>-496.767334</td>\n      <td>-0.065289</td>\n      <td>-0.498032</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>66</td>\n      <td>0.538400</td>\n      <td>1.249432</td>\n      <td>0.413980</td>\n      <td>0.666667</td>\n      <td>0.835452</td>\n      <td>-384.966522</td>\n      <td>-296.144379</td>\n      <td>-1.112077</td>\n      <td>-0.687450</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>67</td>\n      <td>0.344600</td>\n      <td>1.599563</td>\n      <td>0.431938</td>\n      <td>1.000000</td>\n      <td>1.167625</td>\n      <td>-270.479401</td>\n      <td>-255.584457</td>\n      <td>-0.718376</td>\n      <td>-1.030399</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>68</td>\n      <td>0.500800</td>\n      <td>0.670657</td>\n      <td>-0.097955</td>\n      <td>0.833333</td>\n      <td>0.768612</td>\n      <td>-209.968582</td>\n      <td>-276.829498</td>\n      <td>-0.732718</td>\n      <td>-0.596171</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>69</td>\n      <td>0.304300</td>\n      <td>1.827415</td>\n      <td>0.363637</td>\n      <td>1.000000</td>\n      <td>1.463778</td>\n      <td>-508.825836</td>\n      <td>-268.821625</td>\n      <td>-0.737758</td>\n      <td>-0.626680</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>0.684600</td>\n      <td>0.522648</td>\n      <td>0.289005</td>\n      <td>0.500000</td>\n      <td>0.233643</td>\n      <td>-645.671570</td>\n      <td>-312.227905</td>\n      <td>-0.330612</td>\n      <td>-0.371931</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>71</td>\n      <td>0.568000</td>\n      <td>1.375630</td>\n      <td>0.484449</td>\n      <td>0.666667</td>\n      <td>0.891182</td>\n      <td>-248.899033</td>\n      <td>-234.059128</td>\n      <td>-1.400963</td>\n      <td>-1.034823</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>0.728400</td>\n      <td>1.473369</td>\n      <td>0.879203</td>\n      <td>0.666667</td>\n      <td>0.594166</td>\n      <td>-467.468719</td>\n      <td>-284.343842</td>\n      <td>-0.213222</td>\n      <td>-0.464854</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>73</td>\n      <td>0.668400</td>\n      <td>0.820023</td>\n      <td>0.379825</td>\n      <td>0.500000</td>\n      <td>0.440198</td>\n      <td>-412.108490</td>\n      <td>-289.332153</td>\n      <td>-1.001699</td>\n      <td>-0.562359</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>74</td>\n      <td>0.654800</td>\n      <td>0.687911</td>\n      <td>0.354589</td>\n      <td>0.666667</td>\n      <td>0.333322</td>\n      <td>-490.895111</td>\n      <td>-550.230347</td>\n      <td>-1.711452</td>\n      <td>-0.569462</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>0.597600</td>\n      <td>1.853855</td>\n      <td>1.094596</td>\n      <td>0.666667</td>\n      <td>0.759259</td>\n      <td>-540.676819</td>\n      <td>-374.173096</td>\n      <td>-0.844740</td>\n      <td>-1.047970</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>76</td>\n      <td>0.534600</td>\n      <td>1.853928</td>\n      <td>1.096933</td>\n      <td>0.666667</td>\n      <td>0.756995</td>\n      <td>-449.756744</td>\n      <td>-352.441040</td>\n      <td>-0.795595</td>\n      <td>-0.338051</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>77</td>\n      <td>0.297600</td>\n      <td>1.644114</td>\n      <td>0.070616</td>\n      <td>1.000000</td>\n      <td>1.573498</td>\n      <td>-209.519577</td>\n      <td>-226.516357</td>\n      <td>-0.191864</td>\n      <td>-0.022225</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>78</td>\n      <td>1.003900</td>\n      <td>1.193048</td>\n      <td>1.630476</td>\n      <td>0.500000</td>\n      <td>-0.437428</td>\n      <td>-338.359344</td>\n      <td>-245.189026</td>\n      <td>-0.596374</td>\n      <td>0.120198</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>79</td>\n      <td>0.972800</td>\n      <td>0.527060</td>\n      <td>0.943619</td>\n      <td>0.333333</td>\n      <td>-0.416559</td>\n      <td>-337.376831</td>\n      <td>-250.404099</td>\n      <td>-1.062348</td>\n      <td>-1.748564</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>0.910900</td>\n      <td>0.637520</td>\n      <td>0.521598</td>\n      <td>0.500000</td>\n      <td>0.115921</td>\n      <td>-328.815948</td>\n      <td>-183.682068</td>\n      <td>-0.614746</td>\n      <td>-0.946248</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>81</td>\n      <td>0.857600</td>\n      <td>0.989104</td>\n      <td>0.935324</td>\n      <td>0.333333</td>\n      <td>0.053780</td>\n      <td>-556.476624</td>\n      <td>-224.416443</td>\n      <td>-0.413884</td>\n      <td>-0.110754</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>82</td>\n      <td>0.475200</td>\n      <td>1.358469</td>\n      <td>0.433490</td>\n      <td>0.666667</td>\n      <td>0.924978</td>\n      <td>-320.460968</td>\n      <td>-514.876404</td>\n      <td>-0.891835</td>\n      <td>0.462616</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>83</td>\n      <td>0.953100</td>\n      <td>0.395741</td>\n      <td>-0.412445</td>\n      <td>0.500000</td>\n      <td>0.808186</td>\n      <td>-548.165466</td>\n      <td>-467.854340</td>\n      <td>-0.567073</td>\n      <td>-0.499887</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n    <tr>\n      <td>84</td>\n      <td>0.613700</td>\n      <td>1.218506</td>\n      <td>1.050925</td>\n      <td>1.000000</td>\n      <td>0.167580</td>\n      <td>-290.274292</td>\n      <td>-179.446320</td>\n      <td>0.434645</td>\n      <td>-0.327248</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n      <td>No Log</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":117},{"cell_type":"code","source":"# Prepare model for inferencing\nFastLanguageModel.for_inference(dpo_trainer.model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:09:22.499499Z","iopub.execute_input":"2025-08-08T13:09:22.499738Z","iopub.status.idle":"2025-08-08T13:09:22.522698Z","shell.execute_reply.started":"2025-08-08T13:09:22.499721Z","shell.execute_reply":"2025-08-08T13:09:22.522072Z"}},"outputs":[{"execution_count":118,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Qwen3ForCausalLM(\n      (model): Qwen3Model(\n        (embed_tokens): Embedding(151936, 1024, padding_idx=151654)\n        (layers): ModuleList(\n          (0-1): 2 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (2): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (3-26): 24 x Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n          (27): Qwen3DecoderLayer(\n            (self_attn): Qwen3Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=1024, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n              (rotary_emb): LlamaRotaryEmbedding()\n            )\n            (mlp): Qwen3MLP(\n              (gate_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear(\n                (base_layer): Linear(in_features=1024, out_features=3072, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=1024, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=3072, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear(\n                (base_layer): Linear(in_features=3072, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Identity()\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=3072, out_features=8, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=8, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): SiLU()\n            )\n            (input_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n            (post_attention_layernorm): Qwen3RMSNorm((1024,), eps=1e-06)\n          )\n        )\n        (norm): Qwen3RMSNorm((1024,), eps=1e-06)\n        (rotary_emb): LlamaRotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=1024, out_features=151936, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":118},{"cell_type":"markdown","source":"## Prompt\n**how can i develop a habit of drawing daily**\n\n---\n\n## Chosen\nDeveloping a daily habit of drawing can be challenging but with consistent practice and a few tips, it can become an enjoyable and rewarding part of your daily routine. Here are some strategies to help you develop the habit of drawing daily:\n\n1. **Set a specific time**  \n   Allocate a specific time of the day to draw. It could be in the morning, afternoon, or evening. Make drawing a part of your daily routine.\n\n2. **Set a specific duration**  \n   Determine the amount of time you want to spend on drawing each day. It can be as little as 10 minutes or as long as an hour. Be consistent with the duration to help build the habit.\n\n3. **Start small and simple**  \n   Don't try to create a masterpiece every day; start with simple and easy-to-do sketches. Focus on improving your skills gradually.\n\n4. **Use a variety of tools and mediums**  \n   Experiment with different tools like pencils, pens, markers, and different mediums like paper, canvas, or digital apps to keep your drawing practice interesting and engaging.\n\n5. **Take breaks and rest**  \n   Taking breaks and resting after some time of drawing can help you avoid burnout and stay motivated.\n\n6. **Challenge yourself**  \n   Set challenges like drawing objects from memory or a specific subject to improve your skills and keep your drawing practice interesting.\n\n7. **Track your progress**  \n   Keep a record of your daily drawing practice and track your progress. This can be a source of motivation and help you see how far you've come.\n\n> Remember, developing a habit takes time and patience. Stay consistent with your drawing practice, be flexible and open to trying new things, and with time, you'll develop a habit of daily drawing that brings you joy and satisfaction.\n\n---\n\n## Rejected\nAs an AI language model, I cannot personally develop habits for you. But, here are some tips for developing a habit of drawing daily:\n\n1. **Start small**  \n   Start with simple drawings or doodles and gradually increase the complexity of your drawings.\n\n2. **Set a schedule**  \n   Set a specific time every day to draw, and try to stick to it as much as possible.\n\n3. **Make it fun**  \n   Don't pressure yourself to create masterpieces every time you draw. Make it a relaxing and enjoyable experience.\n\n4. **Use resources**  \n   There are many drawing tutorials available online. Use resources like YouTube or online drawing courses to help you improve your skills.\n\n5. **Surround yourself with inspiration**  \n   Expose yourself to a variety of art forms, such as paintings, illustrations, and photographs, to inspire and motivate you.\n\n> Remember, everyone has their own creative style and pace. Just keep practicing and enjoying the process of drawing.\n","metadata":{}},{"cell_type":"markdown","source":"#### After Training DPO model","metadata":{}},{"cell_type":"code","source":"TestInstructModel(model=dpo_trainer.model,\n                  tokenizer=dpo_trainer.tokenizer, \n                  user_query=\"how can i develop a habit of drawing daily\",\n                  enable_thinking=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-08T13:09:22.524064Z","iopub.execute_input":"2025-08-08T13:09:22.524273Z","iopub.status.idle":"2025-08-08T13:09:58.583923Z","shell.execute_reply.started":"2025-08-08T13:09:22.524256Z","shell.execute_reply":"2025-08-08T13:09:58.583374Z"}},"outputs":[{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"name":"stdout","text":"Model Inferencing...\n------------------------------------------------------------------------------------------------------------------------------------------------------\nChat Template:  User: how can i develop a habit of drawing daily\n\n\nBot Response:\nAssistant: Developing a habit of drawing daily can be a rewarding and enjoyable activity, but it requires consistency and a positive mindset. Heres a step-by-step guide to help you establish and maintain this habit:\n\n### 1. **Set a Regular Schedule**\n   - **Start with a few days a week:** This will help you build a routine and avoid feeling overwhelmed.\n   - **Stick to it:** Consistency is key. Even if its just one or two days a week, its better than never.\n\n### 2. **Choose a Time and Place**\n   - **Find a quiet, comfortable spot:** Whether its your desk, a quiet room, or even a dedicated drawing space, make sure its where you can focus without distractions.\n   - **Set a timer:** Use a timer to remind yourself to draw. For example, you could set it to 10 minutes.\n\n### 3. **Create a Routine**\n   - **Start with a warm-up:** Take a few minutes to relax and get into a good mood before starting your drawing session.\n   - **Set a goal:** Decide what you want to draw each day. It could be a specific theme, a new style, or a simple doodle.\n   - **Keep it simple:** Start with a basic drawing and gradually add more complexity as you become more comfortable.\n\n### 4. **Find a Comfortable Drawing Method**\n   - **Use a pencil:** Pencils are versatile and can be used for a variety of styles.\n   - **Use a paper:** Choose a paper that feels comfortable and allows you to draw comfortably.\n   - **Use a sketchbook or canvas:** If youre new to drawing, a sketchbook might be a good starting point.\n\n### 5. **Be Patient and Enjoy the Process**\n   - **Dont get discouraged if you dont see immediate results:** Drawing is a skill that takes time to develop.\n   - **Enjoy the process:** Take a few minutes to relax and enjoy the act of drawing. Its a great way to unwind and clear your mind.\n\n### 6. **Track Your Progress**\n   - **Keep a drawing journal:** Write down what youre drawing, the date, and any notes about the process.\n   - **Celebrate your progress:** When you achieve a new skill or milestone, take a moment to celebrate.\n\n### 7. **Be Mindful of Your Environment**\n   - **Keep your workspace clean and organized:** This will make it easier to focus and draw.\n   - **Avoid distractions:** Turn off your phone, close your eyes, and focus solely on your drawing.\n\n### 8. **Be Open to Experimentation**\n   - **Try new techniques or styles:** Dont be afraid to experiment and find what works best for you.\n   - **Keep a journal of your progress:** Document your new skills and techniques.\n\n### 9. **Reward Yourself**\n   - **Give yourself a small reward:** After a week or two, reward yourself with something nice, like a treat or a short break.\n\n### 10. **Be Consistent**\n   - **Dont give up if you dont see immediate results:** Consistency is key. Over time, youll see improvements in your drawing skills.\n\nBy following these steps, youll be able to develop a habit of drawing daily and enjoy the process of creating and appreciating art.<|endoftext|>\n","output_type":"stream"}],"execution_count":119}]}